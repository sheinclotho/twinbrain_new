"""
TwinBrain V5 ä¸»ç¨‹åº
==================

å›¾åŸç”Ÿæ•°å­—å­ªç”Ÿè„‘è®­ç»ƒç³»ç»Ÿ

ä½¿ç”¨æ–¹æ³•:
    python main.py --config configs/default.yaml
    
æˆ–ç›´æ¥è¿è¡Œ:
    python main.py  # ä½¿ç”¨é»˜è®¤é…ç½®
"""

import argparse
import hashlib
import json
import logging
import os
import sys
from pathlib import Path
from typing import List, Optional
import yaml
import torch
import numpy as np
from torch_geometric.data import HeteroData

# Reduce CUDA memory fragmentation (recommended when reserved >> allocated).
# Set before any CUDA allocations; setdefault preserves user overrides.
os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from data.loaders import BrainDataLoader
from models.graph_native_mapper import GraphNativeBrainMapper
from models.graph_native_system import GraphNativeBrainModel, GraphNativeTrainer
from utils.helpers import setup_logging, set_seed, save_config, create_output_dir


def truncate_timeseries(ts: np.ndarray, max_len: int) -> np.ndarray:
    """Truncate timeseries [..., T] to at most max_len timepoints.

    Prevents CUDA OOM caused by very long EEG/fMRI sequences creating
    multi-GB [N, T, hidden] tensors inside the ST-GCN encoder.
    """
    if ts.shape[-1] > max_len:
        return ts[..., :max_len]
    return ts


def load_config(config_path: str = None) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if config_path is None:
        config_path = Path(__file__).parent / "configs" / "default.yaml"
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config


def prepare_data(config: dict, logger: logging.Logger):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    logger.info("=" * 60)
    logger.info("æ­¥éª¤ 1/4: åŠ è½½æ•°æ®")
    logger.info("=" * 60)
    
    # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
    data_loader = BrainDataLoader(
        data_root=config['data']['root_dir'],
        modalities=config['data']['modalities'],
    )
    
    # è§£æä»»åŠ¡åˆ—è¡¨é…ç½®
    # ä¼˜å…ˆä½¿ç”¨ tasksï¼ˆåˆ—è¡¨ï¼‰ï¼Œå…¼å®¹æ—§ç‰ˆ taskï¼ˆå•å­—ç¬¦ä¸²ï¼‰
    tasks = config['data'].get('tasks')
    if tasks is None:
        legacy_task = config['data'].get('task')
        if legacy_task is not None:
            tasks = [legacy_task]
            logger.info(
                f"ä½¿ç”¨æ—§ç‰ˆ 'task: {legacy_task}' é…ç½®ã€‚"
                f" å»ºè®®è¿ç§»åˆ° 'tasks: [{legacy_task}]'ã€‚"
            )
        # tasks ä»ä¸º None â†’ è‡ªåŠ¨å‘ç°æ‰€æœ‰ä»»åŠ¡
    elif isinstance(tasks, str):
        tasks = [tasks]

    if tasks is None:
        logger.info("tasks: null â†’ è‡ªåŠ¨å‘ç°æ¯ä¸ªè¢«è¯•çš„æ‰€æœ‰ä»»åŠ¡")
    else:
        logger.info(f"å°†åŠ è½½ä»¥ä¸‹ä»»åŠ¡: {tasks}")
    
    # åŠ è½½æ‰€æœ‰è¢«è¯•ï¼ˆå¯è·¨å¤šä»»åŠ¡ï¼‰
    all_data = data_loader.load_all_subjects(
        tasks=tasks,
        max_subjects=config['data'].get('max_subjects'),
    )
    
    if not all_data:
        raise ValueError("æœªåŠ è½½åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„é…ç½®")
    
    logger.info(f"æˆåŠŸåŠ è½½ {len(all_data)} ä¸ªè¢«è¯•-ä»»åŠ¡ç»„åˆ")
    
    return all_data


# â”€â”€ æ—¶é—´çª—å£é»˜è®¤å€¼ï¼ˆç¥ç»å½±åƒç»éªŒå€¼ï¼Œå¯é€šè¿‡é…ç½®è¦†ç›–ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# fMRI: 50 TRs Ã— TRâ‰ˆ2s = 100s â€” è¦†ç›–ä¸€ä¸ªå®Œæ•´æ…¢æ³¢è„‘çŠ¶æ€å‘¨æœŸï¼ˆHutchison 2013ï¼‰
# EEG: 500 pts Ã· 250Hz = 2s â€” è¦†ç›– alpha (8-12 Hz) + beta (13-30 Hz) ä¸»è¦èŠ‚å¾‹
_DEFAULT_FMRI_WINDOW_SIZE = 50
_DEFAULT_EEG_WINDOW_SIZE = 500


def _graph_cache_key(subject_id: str, task: Optional[str], config: dict) -> str:
    """ä¸ºå›¾ç¼“å­˜ç”Ÿæˆç¨³å®šçš„æ–‡ä»¶åã€‚

    æ–‡ä»¶åå†…åµŒå›¾ç›¸å…³é…ç½®å‚æ•°çš„ MD5 çŸ­å“ˆå¸Œï¼ˆ8ä½ï¼‰ï¼Œä¿®æ”¹ atlasã€å›¾æ‹“æ‰‘å‚æ•°æˆ–
    max_seq_len åï¼Œæ—§ç¼“å­˜æ–‡ä»¶åå°†ä¸å†åŒ¹é…ï¼Œç³»ç»Ÿè‡ªåŠ¨é‡å»ºã€‚

    å½“æ—¶é—´çª—å£é‡‡æ ·ï¼ˆwindowed_samplingï¼‰å¯ç”¨æ—¶ï¼Œç¼“å­˜å­˜å‚¨çš„æ˜¯å®Œæ•´ run çš„å›¾ï¼ˆ
    ç”¨å…¨åºåˆ—è®¡ç®—è¿é€šæ€§ï¼‰ï¼Œå¯¹åº”çš„ç¼“å­˜é”®ä¸å« max_seq_lenï¼ˆä¸æˆªæ–­ï¼‰ã€‚
    """
    w_enabled = config.get('windowed_sampling', {}).get('enabled', False)
    relevant = {
        'graph': config.get('graph', {}),
        'atlas': config['data'].get('atlas', {}),
        # åªæœ‰åœ¨ windowed_sampling å…³é—­æ—¶æ‰æˆªæ–­ï¼Œæ­¤æ—¶ max_seq_len å½±å“è¿é€šæ€§ä¼°è®¡
        'max_seq_len': None if w_enabled else config['training'].get('max_seq_len'),
        'modalities': sorted(config['data'].get('modalities', [])),
        'windowed': w_enabled,
    }
    params_hash = hashlib.md5(
        json.dumps(relevant, sort_keys=True).encode()
    ).hexdigest()[:8]
    task_str = task if task else 'notask'
    return f"{subject_id}_{task_str}_{params_hash}.pt"


def extract_windowed_samples(
    full_graph: HeteroData,
    w_cfg: dict,
    logger: logging.Logger,
) -> List[HeteroData]:
    """å°†ä¸€æ¡å®Œæ•´æ‰«æçš„å›¾åˆ‡åˆ†ä¸ºå¤šä¸ªé‡å æ—¶é—´çª—å£æ ·æœ¬ï¼ˆåŠ¨æ€åŠŸèƒ½è¿æ¥ï¼ŒdFCï¼‰ã€‚

    è®¾è®¡ç†å¿µï¼ˆå‚è§ Hutchison 2013; Chang & Glover 2010ï¼‰ï¼š
    - å›¾æ‹“æ‰‘ï¼ˆedge_indexï¼‰= å®Œæ•´ run çš„ç›¸å…³æ€§ â†’ ç¨³å®šçš„ç»“æ„è¿é€šæ€§ä¼°è®¡
    - èŠ‚ç‚¹ç‰¹å¾ï¼ˆxï¼‰= æ—¶é—´çª—å£åˆ‡ç‰‡ â†’ æ¯ä¸ªçª—å£ä»£è¡¨ä¸€æ¬¡è„‘çŠ¶æ€å¿«ç…§
    - å¤šä¸ªé‡å çª—å£ = å¤šä¸ªè®­ç»ƒæ ·æœ¬ï¼Œä¸”æ¯æ ·æœ¬ T = window_size << T_full â†’ æ—  OOM

    ä¸æœ´ç´ æˆªæ–­ï¼ˆmax_seq_lenï¼‰çš„å…³é”®åŒºåˆ«ï¼š
    - æˆªæ–­ï¼šä¸¢å¼ƒ run æœ«å°¾æ•°æ®ï¼Œä¸”ä»…äº§ç”Ÿ 1 ä¸ªè®­ç»ƒæ ·æœ¬
    - çª—å£ï¼šè¦†ç›–å®Œæ•´ runï¼Œäº§ç”Ÿ N_windows ä¸ªæ ·æœ¬ï¼Œæ¯æ ·æœ¬å‡ç”±å®Œæ•´è¿é€šæ€§æ”¯æ’‘

    Args:
        full_graph: å®Œæ•´ run æ„å»ºçš„å¼‚è´¨å›¾ï¼ˆedge_index æ¥è‡ªå…¨åºåˆ—ç›¸å…³æ€§ä¼°è®¡ï¼‰
        w_cfg:      windowed_sampling é…ç½®å­—å…¸
        logger:     æ—¥å¿—è®°å½•å™¨

    Returns:
        HeteroData åˆ—è¡¨ï¼›å…³é—­æ—¶è¿”å› [full_graph]ï¼ˆä¸æ—§è¡Œä¸ºå…¼å®¹ï¼‰
    """
    if not w_cfg.get('enabled', False):
        return [full_graph]

    node_types = full_graph.node_types
    T_per_type = {nt: full_graph[nt].x.shape[1] for nt in node_types}

    # å„æ¨¡æ€çš„çª—å£å¤§å°ï¼ˆå•ä½ï¼šè¯¥æ¨¡æ€çš„æ—¶é—´æ­¥æ•°ï¼‰
    window_sizes: dict = {}
    for nt in node_types:
        ws = w_cfg.get(f'{nt}_window_size')
        if ws is None:
            # ç¥ç»å½±åƒç»éªŒé»˜è®¤å€¼ï¼šfMRI 50 TRs â‰ˆ 100sï¼ˆä¸€ä¸ªè„‘çŠ¶æ€å‘¨æœŸï¼‰ï¼›
            # EEG 500 pts = 2sï¼ˆè¦†ç›– alpha/beta/gamma ä¸»è¦èŠ‚å¾‹ï¼‰
            ws = _DEFAULT_FMRI_WINDOW_SIZE if nt == 'fmri' else _DEFAULT_EEG_WINDOW_SIZE
        window_sizes[nt] = int(ws)

    stride_fraction = w_cfg.get('stride_fraction', 0.5)

    # ä»¥ fMRI ä½œä¸ºå‚è€ƒæ¨¡æ€ï¼ˆæ—¶é—´æ­¥æœ€å°‘ï¼Œé¿å…åˆ†æ•°çª—å£ï¼‰
    # è‹¥æ—  fMRI åˆ™å–èŠ‚ç‚¹æ•°ç¬¬ä¸€é¡¹
    ref_type = 'fmri' if 'fmri' in node_types else node_types[0]
    ws_ref = window_sizes[ref_type]
    T_ref = T_per_type[ref_type]
    stride = max(1, int(ws_ref * stride_fraction))

    if ws_ref >= T_ref:
        # çª—å£è¦†ç›–å®Œæ•´åºåˆ—ï¼šæ— æ³•å†åˆ†å‰²ï¼Œé€€åŒ–ä¸ºåŸå§‹å•æ ·æœ¬
        logger.debug(
            f"çª—å£å¤§å° ({ref_type}: {ws_ref}) â‰¥ åºåˆ—é•¿åº¦ ({T_ref})ï¼Œ"
            f" çª—å£é‡‡æ ·é€€åŒ–ä¸ºå•æ ·æœ¬ã€‚è‹¥éœ€å¤šçª—å£ï¼Œè¯·å‡å° window_size æˆ–"
            f" å¢å¤§åºåˆ—ï¼ˆè®¾ max_seq_len: nullï¼‰ã€‚"
        )
        return [full_graph]

    window_starts = list(range(0, T_ref - ws_ref + 1, stride))

    windows: List[HeteroData] = []
    for t_start_ref in window_starts:
        win = HeteroData()

        # å…±äº«å›¾æ‹“æ‰‘ï¼ˆæ‰€æœ‰çª—å£ä½¿ç”¨ç›¸åŒçš„ edge_indexï¼Œæ¥è‡ªå…¨åºåˆ—è¿é€šæ€§ä¼°è®¡ï¼‰
        for edge_type in full_graph.edge_types:
            win[edge_type].edge_index = full_graph[edge_type].edge_index
            if hasattr(full_graph[edge_type], 'edge_attr'):
                win[edge_type].edge_attr = full_graph[edge_type].edge_attr

        # æŒ‰æ¯”ä¾‹å¯¹é½å„æ¨¡æ€çš„çª—å£åˆ‡ç‰‡
        for nt in node_types:
            T_nt = T_per_type[nt]
            ws_nt = window_sizes[nt]
            # æ ¹æ®å‚è€ƒæ¨¡æ€æ—¶é—´æ­¥æ¯”ä¾‹ï¼Œç­‰æ¯”ä¾‹å®šä½è¯¥æ¨¡æ€çš„èµ·å§‹ç‚¹
            # ä½¿ç”¨ int() è€Œé round()ï¼šæ•°ç»„ç´¢å¼•ç”¨æ•´æ•°æˆªæ–­è¯­ä¹‰æ›´å¯é¢„æœŸ
            t_start_nt = int(t_start_ref * (T_nt / T_ref))
            t_end_nt = t_start_nt + ws_nt

            x_full = full_graph[nt].x  # [N, T, C]
            if t_end_nt > T_nt:
                # æœ«å°¾çª—å£è¶Šç•Œï¼šç”¨é›¶å¡«å……ä¿æŒå›ºå®š T=ws_nt
                x_slice = x_full[:, t_start_nt:, :]
                pad_len = ws_nt - x_slice.shape[1]
                pad = torch.zeros(
                    x_slice.shape[0], pad_len, x_slice.shape[2],
                    dtype=x_slice.dtype,
                )
                x_slice = torch.cat([x_slice, pad], dim=1)
            else:
                x_slice = x_full[:, t_start_nt:t_end_nt, :]

            win[nt].x = x_slice
            # å¤åˆ¶é™æ€å±æ€§ï¼ˆèŠ‚ç‚¹æ•°ã€ç©ºé—´åæ ‡ã€é‡‡æ ·ç‡ï¼‰
            for attr in ('num_nodes', 'pos', 'sampling_rate'):
                if hasattr(full_graph[nt], attr):
                    setattr(win[nt], attr, getattr(full_graph[nt], attr))

        windows.append(win)

    return windows


def build_graphs(all_data, config: dict, logger: logging.Logger):
    """æ„å»ºå›¾ç»“æ„"""
    logger.info("=" * 60)
    logger.info("æ­¥éª¤ 2/4: æ„å»ºå›¾ç»“æ„")
    logger.info("=" * 60)
    
    _MIN_VOLUMES = 10  # Shared threshold for minimum valid fMRI timepoints

    def process_fmri_timeseries(fmri_data, min_volumes=_MIN_VOLUMES):
        """Extract and normalize fMRI timeseries.

        Handles all common input shapes:
        - 4-D [X, Y, Z, T]: raw volumetric fMRI â†’ averaged to [1, T]
        - 3-D [N_rois, T, ?] or [X, Y, T]: reshaped â†’ averaged to [1, T]
        - 2-D [N_rois, T] or [T, N_rois]: pre-parcellated ROI data â†’ ALL ROIs
          kept as separate nodes [N_rois, T] (no averaging).

        Returns (timeseries [N_rois, T], error_or_None).
        """
        if fmri_data.ndim == 4:
            n_volumes = fmri_data.shape[-1]
            if n_volumes < min_volumes:
                return None, f"Too few volumes: {n_volumes} < {min_volumes}"
            # Average all in-mask voxels â€” single timeseries
            fmri_ts = fmri_data.reshape(-1, n_volumes).mean(axis=0)
            fmri_ts = (fmri_ts - fmri_ts.mean()) / (fmri_ts.std() + 1e-8)
            return fmri_ts.reshape(1, -1), None

        elif fmri_data.ndim == 3:
            n_volumes = fmri_data.shape[-1]
            if n_volumes < min_volumes:
                return None, f"Too few volumes: {n_volumes} < {min_volumes}"
            fmri_ts = fmri_data.reshape(-1, n_volumes).mean(axis=0)
            fmri_ts = (fmri_ts - fmri_ts.mean()) / (fmri_ts.std() + 1e-8)
            return fmri_ts.reshape(1, -1), None

        elif fmri_data.ndim == 2:
            # Already ROI timeseries â€” preserve all ROIs as separate graph nodes.
            # Ensure layout is [N_rois, T].
            if fmri_data.shape[0] > fmri_data.shape[1]:
                fmri_data = fmri_data.T
            N_rois, T = fmri_data.shape
            if T < min_volumes:
                return None, f"Too few timepoints: {T} < {min_volumes}"
            # Normalise each ROI independently
            mean = fmri_data.mean(axis=1, keepdims=True)
            std = fmri_data.std(axis=1, keepdims=True) + 1e-8
            return (fmri_data - mean) / std, None

        else:
            return None, f"Unsupported fMRI shape: {fmri_data.shape}"

    def _parcellate_fmri_with_atlas(fmri_img, atlas_path: Path) -> Optional[np.ndarray]:
        """Apply atlas parcellation to extract per-ROI timeseries.

        Uses nilearn NiftiLabelsMasker which handles resampling automatically.
        Returns [N_rois, T] float32 array, or None on failure.

        Why this matters: without parcellation fMRI is collapsed to a single node
        ([1, T]), making graph convolution meaningless.  With the Schaefer200 atlas
        we get 200 anatomically meaningful nodes â€” the actual design intent.
        """
        try:
            try:
                from nilearn.maskers import NiftiLabelsMasker  # nilearn >= 0.10
            except ImportError:
                from nilearn.input_data import NiftiLabelsMasker  # nilearn < 0.10
            masker = NiftiLabelsMasker(
                labels_img=str(atlas_path),
                standardize=True,
                detrend=True,
            )
            roi_ts = masker.fit_transform(fmri_img)  # [T, N_rois]
            if roi_ts.shape[0] < _MIN_VOLUMES:
                logger.warning(
                    f"Atlas parcellation produced only {roi_ts.shape[0]} timepoints; skipping."
                )
                return None
            if roi_ts.shape[1] < 2:
                logger.warning(
                    f"Atlas parcellation produced only {roi_ts.shape[1]} ROIs; skipping."
                )
                return None
            return roi_ts.T.astype(np.float32)  # [N_rois, T]
        except Exception as e:
            logger.warning(f"Atlas parcellation failed ({e}); falling back to single-node fMRI.")
            return None

    # åˆå§‹åŒ–å›¾æ˜ å°„å™¨
    mapper = GraphNativeBrainMapper(
        atlas_name=config['data']['atlas']['name'],
        add_self_loops=config['graph']['add_self_loops'],
        make_undirected=config['graph']['make_undirected'],
        k_nearest_fmri=config['graph'].get('k_nearest_fmri', 20),
        k_nearest_eeg=config['graph'].get('k_nearest_eeg', 10),
        threshold_fmri=config['graph'].get('threshold_fmri', 0.3),
        threshold_eeg=config['graph'].get('threshold_eeg', 0.2),
        device=config['device']['type'],
    )

    # Resolve atlas file path once (relative to project root)
    atlas_file = Path(__file__).parent / config['data']['atlas']['file']
    if atlas_file.exists():
        logger.info(
            f"Atlas parcellation enabled: {atlas_file.name} â†’ up to 200 fMRI ROI nodes"
        )
    else:
        logger.warning(
            f"Atlas file not found at {atlas_file}; fMRI will use single-node fallback."
        )
    
    # â”€â”€ å›¾ç¼“å­˜è®¾ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    cache_cfg = config['data'].get('cache', {})
    cache_enabled = cache_cfg.get('enabled', False)
    cache_dir: Optional[Path] = None
    if cache_enabled:
        cache_dir = Path(cache_cfg.get('dir', 'outputs/graph_cache'))
        try:
            cache_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"å›¾ç¼“å­˜å·²å¯ç”¨: {cache_dir}")
        except OSError as e:
            logger.warning(f"æ— æ³•åˆ›å»ºç¼“å­˜ç›®å½• {cache_dir}: {e}ï¼Œç¼“å­˜å·²ç¦ç”¨")
            cache_dir = None

    # â”€â”€ æ—¶é—´çª—å£é‡‡æ ·é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    w_cfg = config.get('windowed_sampling', {})
    windowed = w_cfg.get('enabled', False)

    # å½“æ—¶é—´çª—å£é‡‡æ ·å¼€å¯æ—¶ï¼Œè¿é€šæ€§ç”±å®Œæ•´ run ä¼°è®¡ï¼ˆä¸æˆªæ–­ï¼‰ï¼›
    # æˆªæ–­ä»…åœ¨å•æ ·æœ¬è®­ç»ƒæ¨¡å¼ï¼ˆwindowed=Falseï¼‰ä¸‹ä¿ç•™ï¼Œç”¨äºé˜² OOMã€‚
    max_seq_len = config['training'].get('max_seq_len', None)
    if windowed:
        if max_seq_len is not None:
            logger.info(
                f"æ—¶é—´çª—å£é‡‡æ ·å·²å¯ç”¨ (fMRI_ws={w_cfg.get('fmri_window_size', 50)}, "
                f"EEG_ws={w_cfg.get('eeg_window_size', 500)}, "
                f"stride={w_cfg.get('stride_fraction', 0.5)}Ã—ws)ã€‚"
                f" å›¾æ„å»ºå°†ä½¿ç”¨å®Œæ•´åºåˆ—ä»¥è·å¾—å¯é è¿é€šæ€§ä¼°è®¡"
                f"ï¼ˆmax_seq_len={max_seq_len} ä»…åœ¨å•æ ·æœ¬æ¨¡å¼ä¸‹ç”Ÿæ•ˆï¼‰ã€‚"
                f" å»ºè®®è®¾ max_seq_len: null ä»¥å®Œå…¨åˆ©ç”¨å…¨åºåˆ—ã€‚"
            )
        else:
            logger.info(
                f"æ—¶é—´çª—å£é‡‡æ ·å·²å¯ç”¨ (fMRI_ws={w_cfg.get('fmri_window_size', 50)}, "
                f"EEG_ws={w_cfg.get('eeg_window_size', 500)}, "
                f"stride={w_cfg.get('stride_fraction', 0.5)}Ã—ws)ã€‚"
                f" å›¾æ„å»ºå°†ä½¿ç”¨å®Œæ•´åºåˆ—ã€‚"
            )
    else:
        if max_seq_len is not None:
            logger.info(f"åºåˆ—æˆªæ–­å·²å¯ç”¨: max_seq_len={max_seq_len} (é˜²æ­¢ CUDA OOM)")

    graphs: List[HeteroData] = []
    n_cached = 0
    n_windows_total = 0
    for subject_data in all_data:
        subject_id = subject_data.get('subject_id', 'unknown')
        task = subject_data.get('task')

        # â”€â”€ å°è¯•ä»ç¼“å­˜åŠ è½½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if cache_dir is not None:
            cache_key = _graph_cache_key(subject_id, task, config)
            cache_path = cache_dir / cache_key
            if cache_path.exists():
                try:
                    full_graph = torch.load(cache_path, map_location='cpu', weights_only=False)
                    win_samples = extract_windowed_samples(full_graph, w_cfg, logger)
                    graphs.extend(win_samples)
                    n_windows_total += len(win_samples)
                    n_cached += 1
                    logger.debug(
                        f"ä»ç¼“å­˜åŠ è½½å›¾: {cache_key}"
                        + (f" â†’ {len(win_samples)} ä¸ªçª—å£" if windowed else "")
                    )
                    continue
                except Exception as e:
                    logger.warning(f"ç¼“å­˜åŠ è½½å¤±è´¥ ({cache_key}): {e}ï¼Œé‡æ–°æ„å»º")

        graph_list = []
        
        # fMRIå›¾
        if 'fmri' in subject_data:
            fmri_data = subject_data['fmri']['data']
            fmri_img = subject_data['fmri'].get('img')   # preprocessed NIfTI object

            # Prefer atlas parcellation: gives [N_rois, T] (e.g. 200 ROI nodes).
            # Fallback to spatial average â†’ [1, T] when atlas unavailable.
            fmri_ts = None
            if atlas_file.exists() and fmri_img is not None:
                fmri_ts = _parcellate_fmri_with_atlas(fmri_img, atlas_file)

            if fmri_ts is None:
                fmri_ts, error = process_fmri_timeseries(fmri_data)
                if error:
                    logger.warning(f"fMRI processing failed: {error}, skipping subject")
                    continue
            
            # æˆªæ–­ä»…åœ¨å•æ ·æœ¬è®­ç»ƒæ¨¡å¼ä¸‹å¯ç”¨ï¼ˆé˜² CUDA OOMï¼‰ã€‚
            # çª—å£æ¨¡å¼ä¸‹ä¸æˆªæ–­ï¼Œä»¥ä½¿è¿é€šæ€§ä¼°è®¡æ¥è‡ªå®Œæ•´ runã€‚
            if not windowed and max_seq_len is not None:
                fmri_ts = truncate_timeseries(fmri_ts, max_seq_len)
            
            logger.debug(f"fMRI timeseries shape: {fmri_ts.shape} â†’ {fmri_ts.shape[0]} nodes")
            fmri_graph = mapper.map_fmri_to_graph(
                timeseries=fmri_ts,
                connectivity_matrix=None,  # è‡ªåŠ¨è®¡ç®—
            )
            graph_list.append(('fmri', fmri_graph))
        
        # EEGå›¾
        if 'eeg' in subject_data:
            eeg_data = subject_data['eeg']['data']  # [n_channels, n_times]
            eeg_ch_names = subject_data['eeg']['ch_names']
            # ä» loader è·å–çœŸå®é‡‡æ ·ç‡å’Œç”µæåæ ‡ï¼ˆç”± EEGPreprocessor é€šè¿‡
            # standard_1020 montage è®¾ç½®ï¼Œå•ä½ mmï¼‰
            eeg_sfreq = subject_data['eeg'].get('sfreq', 250.0)
            eeg_ch_pos = subject_data['eeg'].get('ch_pos', None)  # [N_ch, 3] mm æˆ– None
            
            # Validate EEG data
            if eeg_data.shape[0] < 8:
                logger.warning(f"EEG has too few channels: {eeg_data.shape[0]}, skipping")
                continue
            if eeg_data.shape[1] < 100:
                logger.warning(f"EEG has too few timepoints: {eeg_data.shape[1]}, skipping")
                continue
            if np.isnan(eeg_data).any() or np.isinf(eeg_data).any():
                logger.warning("EEG contains NaN or Inf values, skipping")
                continue
            
            # æˆªæ–­ä»…åœ¨å•æ ·æœ¬è®­ç»ƒæ¨¡å¼ä¸‹å¯ç”¨ï¼ˆé˜² CUDA OOMï¼‰ã€‚
            # çª—å£æ¨¡å¼ä¸‹ä¸æˆªæ–­ï¼Œä»¥ä½¿è¿é€šæ€§ä¼°è®¡æ¥è‡ªå®Œæ•´ runã€‚
            if not windowed and max_seq_len is not None:
                eeg_data = truncate_timeseries(eeg_data, max_seq_len)
            
            eeg_graph = mapper.map_eeg_to_graph(
                timeseries=eeg_data,
                channel_names=eeg_ch_names,
                channel_positions=eeg_ch_pos,
                sampling_rate=eeg_sfreq,
            )
            graph_list.append(('eeg', eeg_graph))
        
        # åˆå¹¶å›¾ - FIX: Properly merge multi-modal graphs
        if len(graph_list) > 0:
            if len(graph_list) == 1:
                # Single modality: use as-is
                built_graph = graph_list[0][1]
            else:
                # Multi-modal: merge into heterograph
                built_graph = HeteroData()
                for modality, graph in graph_list:
                    # Copy node features and structure
                    for key in graph.node_types:
                        built_graph[key].x = graph[key].x
                        if hasattr(graph[key], 'num_nodes'):
                            built_graph[key].num_nodes = graph[key].num_nodes
                        if hasattr(graph[key], 'pos'):
                            built_graph[key].pos = graph[key].pos
                    
                    # Copy edge structure
                    for edge_type in graph.edge_types:
                        built_graph[edge_type].edge_index = graph[edge_type].edge_index
                        if hasattr(graph[edge_type], 'edge_attr'):
                            built_graph[edge_type].edge_attr = graph[edge_type].edge_attr
                
                # è·¨æ¨¡æ€è¾¹ï¼šEEG â†’ fMRI
                # è®¾è®¡ç†å¿µï¼šEEG ç”µæï¼ˆè¾ƒå°‘èŠ‚ç‚¹ï¼‰å‘ fMRI ROIï¼ˆè¾ƒå¤šèŠ‚ç‚¹ï¼‰æŠ•å°„ä¿¡å·ã€‚
                # create_simple_cross_modal_edges ä¼šéªŒè¯ N_eeg < N_fmri å¹¶åœ¨è¿åæ—¶ç»™å‡ºè­¦å‘Šã€‚
                if 'fmri' in built_graph.node_types and 'eeg' in built_graph.node_types:
                    cross_edges = mapper.create_simple_cross_modal_edges(built_graph)
                    if cross_edges is not None:
                        built_graph['eeg', 'projects_to', 'fmri'].edge_index = cross_edges
            
            # â”€â”€ ä¿å­˜åˆ°ç¼“å­˜ï¼ˆå§‹ç»ˆä¿å­˜å®Œæ•´ run å›¾ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if cache_dir is not None:
                try:
                    cache_key = _graph_cache_key(subject_id, task, config)
                    cache_path = cache_dir / cache_key
                    torch.save(built_graph, cache_path)
                    logger.debug(f"å›¾å·²ç¼“å­˜: {cache_key}")
                except Exception as e:
                    logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥ ({subject_id}/{task}): {e}")

            # â”€â”€ åŠ å…¥è®­ç»ƒåˆ—è¡¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # çª—å£æ¨¡å¼ï¼šåˆ‡åˆ†ä¸ºå¤šä¸ªçŸ­çª—å£æ ·æœ¬ï¼›å•æ ·æœ¬æ¨¡å¼ï¼šç›´æ¥åŠ å…¥å®Œæ•´å›¾ã€‚
            if windowed:
                win_samples = extract_windowed_samples(built_graph, w_cfg, logger)
                graphs.extend(win_samples)
                n_windows_total += len(win_samples)
                logger.debug(
                    f"  {subject_id}/{task}: {len(win_samples)} ä¸ªæ—¶é—´çª—å£æ ·æœ¬"
                )
            else:
                graphs.append(built_graph)

    if len(graphs) == 0:
        raise ValueError("No valid graphs constructed. Check data quality and preprocessing.")

    # â”€â”€ æ±‡æ€»æ—¥å¿— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    n_runs = len(all_data)
    if windowed:
        avg_win = n_windows_total / max(n_runs, 1)
        logger.info(
            f"å›¾æ„å»ºå®Œæˆ: {n_runs} æ¡ run â†’ {n_windows_total} ä¸ªæ—¶é—´çª—å£è®­ç»ƒæ ·æœ¬"
            f" (å¹³å‡ {avg_win:.1f} ä¸ªçª—å£/run)"
            + (f"ï¼Œå…¶ä¸­ {n_cached} æ¡ run æ¥è‡ªç¼“å­˜" if n_cached else "")
        )
    elif n_cached > 0:
        logger.info(
            f"å›¾æ„å»ºå®Œæˆ: {len(graphs)} ä¸ªå›¾"
            f" (å…¶ä¸­ {n_cached} ä¸ªæ¥è‡ªç¼“å­˜ï¼Œ{len(graphs) - n_cached} ä¸ªæ–°å»º)"
        )
    else:
        logger.info(f"æˆåŠŸæ„å»º {len(graphs)} ä¸ªå›¾")

    return graphs, mapper


def create_model(config: dict, logger: logging.Logger):
    """åˆ›å»ºæ¨¡å‹"""
    logger.info("=" * 60)
    logger.info("æ­¥éª¤ 3/4: åˆ›å»ºæ¨¡å‹")
    logger.info("=" * 60)
    
    # ç¡®å®šèŠ‚ç‚¹å’Œè¾¹ç±»å‹
    node_types = config['data']['modalities']
    edge_types = []
    
    for modality in node_types:
        edge_types.append((modality, 'connects', modality))
    
    # è·¨æ¨¡æ€è¾¹ï¼šè®¾è®¡ç†å¿µæ˜¯ EEG â†’ fMRI
    # EEG ç”µæï¼ˆé€šå¸¸ 32â€“64 é€šé“ï¼‰èŠ‚ç‚¹æ•° < fMRI ROIï¼ˆå¦‚ Schaefer200 çš„ 200 ä¸ªï¼‰ï¼Œ
    # å› æ­¤ç”± EEG å‘ fMRI æŠ•å°„æ¶ˆæ¯ç¬¦åˆ"å°‘èŠ‚ç‚¹å‘å¤šèŠ‚ç‚¹ä¼ æ’­"çš„å›¾å·ç§¯è¯­ä¹‰ã€‚
    # ä½¿ç”¨æ¨¡æ€åè€Œéä½ç½®ç´¢å¼•ï¼Œä¿è¯ä¸å— config['data']['modalities'] é¡ºåºå½±å“ã€‚
    if 'eeg' in node_types and 'fmri' in node_types:
        edge_types.append(('eeg', 'projects_to', 'fmri'))
    elif len(node_types) > 1:
        # é EEG/fMRI æ¨¡æ€ç»„åˆçš„é€šç”¨å›é€€
        edge_types.append((node_types[0], 'projects_to', node_types[1]))
    
    # è¾“å…¥é€šé“
    in_channels_dict = {modality: 1 for modality in node_types}
    
    # åˆ›å»ºæ¨¡å‹
    model = GraphNativeBrainModel(
        node_types=node_types,
        edge_types=edge_types,
        in_channels_dict=in_channels_dict,
        hidden_channels=config['model']['hidden_channels'],
        num_encoder_layers=config['model']['num_encoder_layers'],
        num_decoder_layers=config['model']['num_decoder_layers'],
        use_prediction=config['model']['use_prediction'],
        prediction_steps=config['model']['prediction_steps'],
        dropout=config['model']['dropout'],
        loss_type=config['model'].get('loss_type', 'mse'),
        use_gradient_checkpointing=config['training'].get('use_gradient_checkpointing', False),
    )
    
    logger.info(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    return model


def log_training_summary(
    config: dict,
    graphs: list,
    model,
    logger: logging.Logger,
) -> None:
    """å¯åŠ¨è®­ç»ƒå‰æ‰“å°ä¸€æ¬¡äººç±»å¯è¯»çš„é…ç½®æ ¸å¯¹è¡¨ã€‚

    ç›®çš„ï¼šè®©ä»»ä½•äººï¼ˆä¸éœ€è¦äº†è§£ä»£ç ç»†èŠ‚ï¼‰åœ¨çœ‹åˆ°æ—¥å¿—çš„ç¬¬ä¸€çœ¼å°±èƒ½
    ç¡®è®¤æ•°æ®å¤„ç†æ–¹å¼æ˜¯å¦ç¬¦åˆé¢„æœŸï¼Œé¿å…"é»˜é»˜åœ°ç”¨äº†é”™è¯¯å‚æ•°è®­ç»ƒå®Œ
    æ‰å‘ç°"çš„æƒ…å†µã€‚

    ä¿¡æ¯æ¥æºï¼š
    - ä¼˜å…ˆè¯»å– graphs[0] ä¸­çš„å®é™…è¿è¡Œæ—¶å€¼ï¼ˆèŠ‚ç‚¹æ•°ã€åºåˆ—é•¿åº¦ç­‰ï¼‰ï¼Œ
      è€Œä¸æ˜¯ config ä¸­çš„æœŸæœ›å€¼â€”â€”ä¸¤è€…å¯èƒ½å› æ•°æ®è´¨é‡é—®é¢˜è€Œä¸åŒã€‚
    - è¯»å– config è·å–æ¨¡å‹ç»“æ„å’Œè®­ç»ƒè¶…å‚æ•°ã€‚
    """
    sep = "=" * 60

    logger.info(sep)
    logger.info("ğŸ“‹ è®­ç»ƒé…ç½®æ ¸å¯¹è¡¨ (Training Configuration Summary)")
    logger.info(sep)

    # â”€â”€ ä»ç¬¬ä¸€ä¸ªå›¾æå–è¿è¡Œæ—¶å®é™…å€¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    g = graphs[0] if graphs else None
    modalities = config['data'].get('modalities', [])

    logger.info("ã€æ•°æ®ã€‘")

    if g is not None:
        # EEG
        if 'eeg' in getattr(g, 'node_types', []):
            eeg_x = g['eeg'].x  # [N, T, C]
            N_eeg, T_eeg = eeg_x.shape[0], eeg_x.shape[1]
            has_eeg_pos = (
                hasattr(g['eeg'], 'pos')
                and g['eeg'].pos is not None
                and g['eeg'].pos.shape[0] > 0
            )
            # sampling_rate is always written by map_eeg_to_graph; the fallback
            # here matches that function's default (250 Hz) for robustness.
            eeg_sr = getattr(g['eeg'], 'sampling_rate', 250.0)
            logger.info(
                f"  EEG  : {N_eeg} ä¸ªç”µæé€šé“ | "
                f"é‡‡æ ·ç‡: {eeg_sr:.1f} Hz | "
                f"åºåˆ—é•¿åº¦: {T_eeg} ä¸ªæ—¶é—´ç‚¹"
            )
            pos_note = (
                "å·²æ‰¾åˆ° (æ¥è‡ª MNE standard_1020 montageï¼Œå•ä½ mm)"
                if has_eeg_pos
                else "æœªæ‰¾åˆ° â†’ å°†ä½¿ç”¨éšæœºè·¨æ¨¡æ€è¿æ¥ï¼ˆéè·ç¦»åŠ æƒï¼‰"
            )
            logger.info(f"         ç”µæåæ ‡: {pos_note}")

        # fMRI
        if 'fmri' in getattr(g, 'node_types', []):
            fmri_x = g['fmri'].x  # [N, T, C]
            N_fmri, T_fmri = fmri_x.shape[0], fmri_x.shape[1]
            # sampling_rate is always written by map_fmri_to_graph; the fallback
            # here matches that function's default (0.5 Hz = TR 2 s) for robustness.
            fmri_sr = getattr(g['fmri'], 'sampling_rate', 0.5)
            tr_sec = 1.0 / fmri_sr if fmri_sr > 0 else float('nan')
            atlas_used = N_fmri > 1
            atlas_note = (
                f"å·²å¯ç”¨ ({config['data']['atlas']['name']}, {N_fmri} ä¸ª ROI èŠ‚ç‚¹)"
                if atlas_used
                else f"æœªå¯ç”¨ â†’ å•èŠ‚ç‚¹å›é€€ (N_fmri={N_fmri}ï¼Œç©ºé—´ä¿¡æ¯å·²ä¸¢å¤±)"
            )
            logger.info(
                f"  fMRI : {N_fmri} ä¸ª ROI èŠ‚ç‚¹ | "
                f"é‡‡æ ·ç‡: {fmri_sr:.3g} Hz (TRâ‰ˆ{tr_sec:.1f}s) | "
                f"åºåˆ—é•¿åº¦: {T_fmri} ä¸ªæ—¶é—´ç‚¹"
            )
            logger.info(f"         å›¾è°±åˆ†åŒº: {atlas_note}")
            if not atlas_used:
                logger.info(
                    f"  âš ï¸   fMRI åªæœ‰ {N_fmri} ä¸ªèŠ‚ç‚¹ï¼å›¾å·ç§¯æ— æ³•æå–ç©ºé—´ä¿¡æ¯ã€‚"
                    f" è¯·æ£€æŸ¥ atlas æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ã€nilearn æ˜¯å¦å·²å®‰è£…ã€‚"
                )

        # è·¨æ¨¡æ€è¾¹
        if (
            'eeg' in getattr(g, 'node_types', [])
            and 'fmri' in getattr(g, 'node_types', [])
        ):
            cross_edge_type = ('eeg', 'projects_to', 'fmri')
            if cross_edge_type in getattr(g, 'edge_types', []):
                n_cross = g[cross_edge_type].edge_index.shape[1]
                logger.info(
                    f"  è·¨æ¨¡æ€è¾¹ (EEGâ†’fMRI): {n_cross} æ¡"
                    f" | æ–¹å‘æ­£ç¡® (N_eeg={N_eeg} < N_fmri={N_fmri})"
                    if N_eeg < N_fmri
                    else
                    f"  è·¨æ¨¡æ€è¾¹ (EEGâ†’fMRI): {n_cross} æ¡"
                    f"  âš ï¸  N_eeg({N_eeg}) â‰¥ N_fmri({N_fmri}), è¯·æ£€æŸ¥å›¾è°±åŠ è½½"
                )
            else:
                logger.info("  è·¨æ¨¡æ€è¾¹ (EEGâ†’fMRI): æœªå»ºç«‹")
    else:
        logger.info("  (æ— å›¾æ•°æ®å¯ä¾›åˆ†æ)")

    max_seq = config['training'].get('max_seq_len')
    if max_seq:
        logger.info(f"  åºåˆ—æˆªæ–­ max_seq_len: {max_seq} (é˜²æ­¢ CUDA OOM)")
    else:
        logger.info("  åºåˆ—æˆªæ–­: æœªå¯ç”¨ (è‹¥åºåˆ—è¿‡é•¿å¯èƒ½ OOMï¼Œå»ºè®®è®¾ç½® max_seq_len)")

    # â”€â”€ æ¨¡å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("ã€æ¨¡å‹ã€‘")
    logger.info(
        f"  éšå±‚ç»´åº¦: {config['model']['hidden_channels']} | "
        f"ç¼–ç å±‚æ•°: {config['model']['num_encoder_layers']} | "
        f"è§£ç å±‚æ•°: {config['model']['num_decoder_layers']}"
    )
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"  æ€»å‚æ•°é‡: {total_params:,}")
    logger.info(f"  æŸå¤±å‡½æ•°: {config['model'].get('loss_type', 'mse')}")

    # â”€â”€ è®­ç»ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("ã€è®­ç»ƒã€‘")
    device = config['device']['type']
    use_amp = config['device'].get('use_amp', True)
    use_gc = config['training'].get('use_gradient_checkpointing', False)
    use_al = config['training'].get('use_adaptive_loss', True)
    lr = config['training']['learning_rate']
    logger.info(
        f"  è®¾å¤‡: {device} | "
        f"æ··åˆç²¾åº¦(AMP): {'æ˜¯' if use_amp else 'å¦'} | "
        f"æ¢¯åº¦æ£€æŸ¥ç‚¹: {'æ˜¯' if use_gc else 'å¦'}"
    )
    logger.info(
        f"  å­¦ä¹ ç‡: {lr} | "
        f"è‡ªé€‚åº”æŸå¤±æƒé‡: {'æ˜¯' if use_al else 'å¦'}"
    )

    logger.info(sep)
    logger.info("âš ï¸  è¯·æ ¸å¯¹ä»¥ä¸Šå‚æ•°æ˜¯å¦ä¸æ‚¨çš„å®éªŒé¢„æœŸä¸€è‡´ï¼Œå†ç»§ç»­è®­ç»ƒã€‚")
    logger.info(sep)


def train_model(model, graphs, config: dict, logger: logging.Logger):
    """è®­ç»ƒæ¨¡å‹"""
    logger.info("=" * 60)
    logger.info("æ­¥éª¤ 4/4: è®­ç»ƒæ¨¡å‹")
    logger.info("=" * 60)
    
    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›† - Ensure both train and validation have samples
    if len(graphs) < 2:
        logger.error(f"âŒ æ•°æ®ä¸è¶³: éœ€è¦è‡³å°‘2ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œä½†åªæœ‰ {len(graphs)} ä¸ªæ ·æœ¬")
        logger.error("æç¤º: è¯·å¢åŠ æ•°æ®é‡æˆ–è°ƒæ•´ max_subjects é…ç½®")
        raise ValueError(f"éœ€è¦è‡³å°‘2ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ,ä½†åªæœ‰ {len(graphs)} ä¸ªã€‚è¯·æ£€æŸ¥æ•°æ®é…ç½®ã€‚")
    
    # Use at least 10% or 1 sample for validation, ensure both train and val have at least 1
    min_val_samples = max(1, len(graphs) // 10)
    n_train = len(graphs) - min_val_samples
    
    # Safety check: ensure both sets have at least 1 sample
    if n_train < 1:
        n_train = 1
        min_val_samples = len(graphs) - 1
    
    train_graphs = graphs[:n_train]
    val_graphs = graphs[n_train:]
    
    logger.info(f"è®­ç»ƒé›†: {len(train_graphs)} ä¸ªæ ·æœ¬")
    logger.info(f"éªŒè¯é›†: {len(val_graphs)} ä¸ªæ ·æœ¬")
    
    if len(train_graphs) < 5:
        logger.warning("âš ï¸ è®­ç»ƒæ ·æœ¬è¾ƒå°‘ï¼Œæ¨¡å‹å¯èƒ½è¿‡æ‹Ÿåˆã€‚å»ºè®®ä½¿ç”¨æ›´å¤šæ•°æ®ã€‚")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    logger.info("æ­£åœ¨åˆå§‹åŒ–è®­ç»ƒå™¨...")
    if config['device'].get('use_torch_compile', True):
        logger.info("âš™ï¸ torch.compile() å·²å¯ç”¨ï¼Œé¦–æ¬¡è®­ç»ƒå¯èƒ½éœ€è¦é¢å¤–æ—¶é—´è¿›è¡Œæ¨¡å‹ç¼–è¯‘...")
    
    trainer = GraphNativeTrainer(
        model=model,
        node_types=config['data']['modalities'],
        learning_rate=config['training']['learning_rate'],
        weight_decay=config['training']['weight_decay'],
        use_adaptive_loss=config['training']['use_adaptive_loss'],
        use_eeg_enhancement=config['training']['use_eeg_enhancement'],
        use_amp=config['device'].get('use_amp', True),
        use_gradient_checkpointing=config['training'].get('use_gradient_checkpointing', False),
        use_scheduler=config['training'].get('use_scheduler', True),
        scheduler_type=config['training'].get('scheduler_type', 'cosine'),
        use_torch_compile=config['device'].get('use_torch_compile', True),
        compile_mode=config['device'].get('compile_mode', 'reduce-overhead'),
        device=config['device']['type'],
    )
    logger.info("âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    logger.info("=" * 60)
    logger.info("å¼€å§‹è®­ç»ƒå¾ªç¯")
    logger.info("=" * 60)
    
    # è®­ç»ƒå¾ªç¯
    import time
    best_val_loss = float('inf')
    patience_counter = 0
    no_improvement_warning_shown = False
    epoch_times = []
    
    for epoch in range(1, config['training']['num_epochs'] + 1):
        epoch_start_time = time.time()
        
        # è®­ç»ƒ
        train_loss = trainer.train_epoch(train_graphs, epoch=epoch, total_epochs=config['training']['num_epochs'])
        
        epoch_time = time.time() - epoch_start_time
        epoch_times.append(epoch_time)
        
        # Estimate remaining time (after first few epochs)
        if len(epoch_times) >= 3:
            avg_epoch_time = sum(epoch_times[-5:]) / len(epoch_times[-5:])  # Use last 5 epochs
            remaining_epochs = config['training']['num_epochs'] - epoch
            eta_seconds = avg_epoch_time * remaining_epochs
            eta_minutes = eta_seconds / 60
            if eta_minutes < 60:
                eta_str = f"{eta_minutes:.1f} åˆ†é’Ÿ"
            else:
                eta_str = f"{eta_minutes/60:.1f} å°æ—¶"
        else:
            eta_str = "è®¡ç®—ä¸­..."
        
        # Memory monitoring every 10 epochs
        if epoch % 10 == 0 and torch.cuda.is_available():
            allocated_gb = torch.cuda.memory_allocated() / 1e9
            reserved_gb = torch.cuda.memory_reserved() / 1e9
            logger.info(f"  ğŸ’¾ GPU Memory: allocated={allocated_gb:.2f} GB, reserved={reserved_gb:.2f} GB")
        
        # Check for NaN loss
        if np.isnan(train_loss) or np.isinf(train_loss):
            logger.error(f"âŒ Training loss is NaN/Inf at epoch {epoch}. Stopping training.")
            raise ValueError("Training diverged: loss is NaN or Inf")
        
        # éªŒè¯
        if epoch % config['training']['val_frequency'] == 0:
            val_loss = trainer.validate(val_graphs)
            
            # Step scheduler based on validation loss (for ReduceLROnPlateau)
            trainer.step_scheduler_on_validation(val_loss)
            
            # Check for NaN validation loss
            if np.isnan(val_loss) or np.isinf(val_loss):
                logger.error(f"âŒ Validation loss is NaN/Inf at epoch {epoch}. Stopping training.")
                raise ValueError("Validation diverged: loss is NaN or Inf")
            
            logger.info(
                f"âœ“ Epoch {epoch}/{config['training']['num_epochs']}: "
                f"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, "
                f"time={epoch_time:.1f}s, ETA={eta_str}"
            )
            
            # Warn if no improvement after many epochs
            if epoch >= 50 and best_val_loss == float('inf') and not no_improvement_warning_shown:
                logger.warning("âš ï¸ No improvement in validation loss after 50 epochs. Check data quality and hyperparameters.")
                no_improvement_warning_shown = True
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                improvement = (best_val_loss - val_loss) / best_val_loss * 100 if best_val_loss != float('inf') else 100
                best_val_loss = val_loss
                patience_counter = 0
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                output_dir = Path(config['output']['output_dir'])
                checkpoint_path = output_dir / "best_model.pt"
                trainer.save_checkpoint(checkpoint_path, epoch)
                if improvement != 100:
                    logger.info(f"  ğŸ¯ ä¿å­˜æœ€ä½³æ¨¡å‹: val_loss={val_loss:.4f} (æå‡ {improvement:.1f}%)")
                else:
                    logger.info(f"  ğŸ¯ ä¿å­˜æœ€ä½³æ¨¡å‹: val_loss={val_loss:.4f}")
            else:
                patience_counter += 1
            
            # æ—©åœ
            if patience_counter >= config['training']['early_stopping_patience']:
                logger.info(f"â¹ï¸ æ—©åœè§¦å‘: {patience_counter} ä¸ªepochæ— æ”¹è¿›")
                break
        else:
            logger.info(
                f"âœ“ Epoch {epoch}/{config['training']['num_epochs']}: "
                f"train_loss={train_loss:.4f}, time={epoch_time:.1f}s, ETA={eta_str}"
            )
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % config['training']['save_frequency'] == 0:
            output_dir = Path(config['output']['output_dir'])
            checkpoint_path = output_dir / f"checkpoint_epoch_{epoch}.pt"
            trainer.save_checkpoint(checkpoint_path, epoch)
    
    logger.info("è®­ç»ƒå®Œæˆ!")
    logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‚æ•°
    parser = argparse.ArgumentParser(description='TwinBrain V5 è®­ç»ƒç³»ç»Ÿ')
    parser.add_argument(
        '--config',
        type=str,
        default=None,
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (default: configs/default.yaml)'
    )
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='éšæœºç§å­ (default: 42)'
    )
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = create_output_dir(
        config['output']['output_dir'],
        config['output']['experiment_name']
    )
    config['output']['output_dir'] = str(output_dir)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(
        output_dir / "training.log",
        level=config['output']['log_level']
    )
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # ä¿å­˜é…ç½®
    save_config(config, output_dir / "config.yaml")
    
    # æ‰“å°é…ç½®
    logger.info("=" * 60)
    logger.info("TwinBrain V5 - å›¾åŸç”Ÿæ•°å­—å­ªç”Ÿè„‘è®­ç»ƒç³»ç»Ÿ")
    logger.info("=" * 60)
    logger.info(f"é…ç½®æ–‡ä»¶: {args.config or 'configs/default.yaml'}")
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    logger.info(f"è®¾å¤‡: {config['device']['type']}")
    logger.info(f"éšæœºç§å­: {args.seed}")
    logger.info("=" * 60)
    
    try:
        # æ­¥éª¤1: å‡†å¤‡æ•°æ®
        all_data = prepare_data(config, logger)
        
        # æ­¥éª¤2: æ„å»ºå›¾
        graphs, mapper = build_graphs(all_data, config, logger)
        
        # æ­¥éª¤3: åˆ›å»ºæ¨¡å‹
        model = create_model(config, logger)
        
        # å¯åŠ¨å‰æ‰“å°ä¸€æ¬¡äººç±»å¯è¯»çš„é…ç½®æ ¸å¯¹è¡¨ï¼Œæ–¹ä¾¿å¿«é€ŸéªŒè¯å‚æ•°
        log_training_summary(config, graphs, model, logger)
        
        # æ­¥éª¤4: è®­ç»ƒ
        train_model(model, graphs, config, logger)
        
        logger.info("=" * 60)
        logger.info("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"âŒ è¿è¡Œå¤±è´¥: {e}", exc_info=True)
        raise


if __name__ == '__main__':
    main()
