"""
TwinBrain V5 ä¸»ç¨‹åº
==================

å›¾åŸç”Ÿæ•°å­—å­ªç”Ÿè„‘è®­ç»ƒç³»ç»Ÿ

ä½¿ç”¨æ–¹æ³•:
    python main.py --config configs/default.yaml
    
æˆ–ç›´æ¥è¿è¡Œ:
    python main.py  # ä½¿ç”¨é»˜è®¤é…ç½®
"""

import argparse
import hashlib
import json
import logging
import os
import random
import sys
import time
from collections import defaultdict, Counter
from pathlib import Path
from typing import List, Optional
import yaml
import torch
import numpy as np
from torch_geometric.data import HeteroData

# Reduce CUDA memory fragmentation (recommended when reserved >> allocated).
# Set before any CUDA allocations; setdefault preserves user overrides.
os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from data.loaders import BrainDataLoader
from models.graph_native_mapper import GraphNativeBrainMapper
from models.graph_native_system import GraphNativeBrainModel, GraphNativeTrainer
from utils.helpers import setup_logging, set_seed, save_config, create_output_dir


def truncate_timeseries(ts: np.ndarray, max_len: int) -> np.ndarray:
    """Truncate timeseries [..., T] to at most max_len timepoints.

    Prevents CUDA OOM caused by very long EEG/fMRI sequences creating
    multi-GB [N, T, hidden] tensors inside the ST-GCN encoder.
    """
    if ts.shape[-1] > max_len:
        return ts[..., :max_len]
    return ts


def load_config(config_path: str = None) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if config_path is None:
        config_path = Path(__file__).parent / "configs" / "default.yaml"
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config


# â”€â”€ æ—¶é—´çª—å£é»˜è®¤å€¼ï¼ˆç¥ç»å½±åƒç»éªŒå€¼ï¼Œå¯é€šè¿‡é…ç½®è¦†ç›–ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# fMRI: 50 TRs Ã— TRâ‰ˆ2s = 100s â€” è¦†ç›–ä¸€ä¸ªå®Œæ•´æ…¢æ³¢è„‘çŠ¶æ€å‘¨æœŸï¼ˆHutchison 2013ï¼‰
# EEG: 500 pts Ã· 250Hz = 2s â€” è¦†ç›– alpha (8-12 Hz) + beta (13-30 Hz) ä¸»è¦èŠ‚å¾‹
_DEFAULT_FMRI_WINDOW_SIZE = 50
_DEFAULT_EEG_WINDOW_SIZE = 500


def _graph_cache_key(subject_id: str, task: Optional[str], config: dict) -> str:
    """ä¸ºå›¾ç¼“å­˜ç”Ÿæˆç¨³å®šçš„æ–‡ä»¶åã€‚

    æ–‡ä»¶åå†…åµŒå›¾ç›¸å…³é…ç½®å‚æ•°çš„ MD5 çŸ­å“ˆå¸Œï¼ˆ8ä½ï¼‰ï¼Œä¿®æ”¹ atlasã€å›¾æ‹“æ‰‘å‚æ•°æˆ–
    max_seq_len åï¼Œæ—§ç¼“å­˜æ–‡ä»¶åå°†ä¸å†åŒ¹é…ï¼Œç³»ç»Ÿè‡ªåŠ¨é‡å»ºã€‚

    å½“æ—¶é—´çª—å£é‡‡æ ·ï¼ˆwindowed_samplingï¼‰å¯ç”¨æ—¶ï¼Œç¼“å­˜å­˜å‚¨çš„æ˜¯å®Œæ•´ run çš„å›¾ï¼ˆ
    ç”¨å…¨åºåˆ—è®¡ç®—è¿é€šæ€§ï¼‰ï¼Œå¯¹åº”çš„ç¼“å­˜é”®ä¸å« max_seq_lenï¼ˆä¸æˆªæ–­ï¼‰ã€‚
    """
    w_enabled = config.get('windowed_sampling', {}).get('enabled', False)
    relevant = {
        'graph': config.get('graph', {}),
        'atlas': config['data'].get('atlas', {}),
        # åªæœ‰åœ¨ windowed_sampling å…³é—­æ—¶æ‰æˆªæ–­ï¼Œæ­¤æ—¶ max_seq_len å½±å“è¿é€šæ€§ä¼°è®¡
        'max_seq_len': None if w_enabled else config['training'].get('max_seq_len'),
        'modalities': sorted(config['data'].get('modalities', [])),
        'windowed': w_enabled,
        # DTI å¼€å…³å½±å“å›¾ç»“æ„ï¼ˆæ˜¯å¦å« ('fmri','structural','fmri') è¾¹ï¼‰ï¼›
        # ä¿®æ”¹æ­¤é€‰é¡¹å¿…é¡»ä½¿æ—§ç¼“å­˜å¤±æ•ˆï¼Œå¦åˆ™åˆ‡æ¢ DTI åä»åŠ è½½æ—  DTI è¾¹çš„æ—§å›¾ã€‚
        'dti_structural_edges': config['data'].get('dti_structural_edges', False),
        # fMRI æ¡ä»¶æ—¶é—´æ®µæˆªå–ï¼šä¿®æ”¹æ­¤é€‰é¡¹æ”¹å˜ fMRI èŠ‚ç‚¹ç‰¹å¾çš„æ—¶é—´ç»´åº¦å’Œè¿é€šæ€§ä¼°è®¡ï¼Œ
        # å¿…é¡»ä½¿æ—§ç¼“å­˜å¤±æ•ˆï¼Œå¦åˆ™ä½¿ç”¨é”™è¯¯æ—¶é—´æ®µçš„å›¾å‚ä¸è®­ç»ƒã€‚
        'fmri_condition_bounds': config['data'].get('fmri_condition_bounds'),
        # EEG è¿é€šæ€§æ–¹æ³•ï¼š'correlation' vs 'coherence' äº§ç”Ÿä¸åŒ edge_index/edge_attrï¼Œ
        # åˆ‡æ¢æ–¹æ³•å¿…é¡»ä½¿æ—§ç¼“å­˜å¤±æ•ˆï¼Œå¦åˆ™ coherence æ¨¡å¼ä¼šä½¿ç”¨ correlation æƒé‡çš„æ—§å›¾ã€‚
        'eeg_connectivity_method': config['graph'].get('eeg_connectivity_method', 'correlation'),
    }
    params_hash = hashlib.md5(
        json.dumps(relevant, sort_keys=True).encode()
    ).hexdigest()[:8]
    task_str = task if task else 'notask'
    return f"{subject_id}_{task_str}_{params_hash}.pt"


def extract_windowed_samples(
    full_graph: HeteroData,
    w_cfg: dict,
    logger: logging.Logger,
) -> List[HeteroData]:
    """å°†ä¸€æ¡å®Œæ•´æ‰«æçš„å›¾åˆ‡åˆ†ä¸ºå¤šä¸ªé‡å æ—¶é—´çª—å£æ ·æœ¬ï¼ˆåŠ¨æ€åŠŸèƒ½è¿æ¥ï¼ŒdFCï¼‰ã€‚

    è®¾è®¡ç†å¿µï¼ˆå‚è§ Hutchison 2013; Chang & Glover 2010ï¼‰ï¼š
    - å›¾æ‹“æ‰‘ï¼ˆedge_indexï¼‰= å®Œæ•´ run çš„ç›¸å…³æ€§ â†’ ç¨³å®šçš„ç»“æ„è¿é€šæ€§ä¼°è®¡
    - èŠ‚ç‚¹ç‰¹å¾ï¼ˆxï¼‰= æ—¶é—´çª—å£åˆ‡ç‰‡ â†’ æ¯ä¸ªçª—å£ä»£è¡¨ä¸€æ¬¡è„‘çŠ¶æ€å¿«ç…§
    - å¤šä¸ªé‡å çª—å£ = å¤šä¸ªè®­ç»ƒæ ·æœ¬ï¼Œä¸”æ¯æ ·æœ¬ T = window_size << T_full â†’ æ—  OOM

    ä¸æœ´ç´ æˆªæ–­ï¼ˆmax_seq_lenï¼‰çš„å…³é”®åŒºåˆ«ï¼š
    - æˆªæ–­ï¼šä¸¢å¼ƒ run æœ«å°¾æ•°æ®ï¼Œä¸”ä»…äº§ç”Ÿ 1 ä¸ªè®­ç»ƒæ ·æœ¬
    - çª—å£ï¼šè¦†ç›–å®Œæ•´ runï¼Œäº§ç”Ÿ N_windows ä¸ªæ ·æœ¬ï¼Œæ¯æ ·æœ¬å‡ç”±å®Œæ•´è¿é€šæ€§æ”¯æ’‘

    Args:
        full_graph: å®Œæ•´ run æ„å»ºçš„å¼‚è´¨å›¾ï¼ˆedge_index æ¥è‡ªå…¨åºåˆ—ç›¸å…³æ€§ä¼°è®¡ï¼‰
        w_cfg:      windowed_sampling é…ç½®å­—å…¸
        logger:     æ—¥å¿—è®°å½•å™¨

    Returns:
        HeteroData åˆ—è¡¨ï¼›å…³é—­æ—¶è¿”å› [full_graph]ï¼ˆä¸æ—§è¡Œä¸ºå…¼å®¹ï¼‰
    """
    if not w_cfg.get('enabled', False):
        return [full_graph]

    node_types = full_graph.node_types
    T_per_type = {nt: full_graph[nt].x.shape[1] for nt in node_types}

    # å„æ¨¡æ€çš„çª—å£å¤§å°ï¼ˆå•ä½ï¼šè¯¥æ¨¡æ€çš„æ—¶é—´æ­¥æ•°ï¼‰
    window_sizes: dict = {}
    for nt in node_types:
        ws = w_cfg.get(f'{nt}_window_size')
        if ws is None:
            # ç¥ç»å½±åƒç»éªŒé»˜è®¤å€¼ï¼šfMRI 50 TRs â‰ˆ 100sï¼ˆä¸€ä¸ªè„‘çŠ¶æ€å‘¨æœŸï¼‰ï¼›
            # EEG 500 pts = 2sï¼ˆè¦†ç›– alpha/beta/gamma ä¸»è¦èŠ‚å¾‹ï¼‰
            ws = _DEFAULT_FMRI_WINDOW_SIZE if nt == 'fmri' else _DEFAULT_EEG_WINDOW_SIZE
        window_sizes[nt] = int(ws)

    stride_fraction = w_cfg.get('stride_fraction', 0.5)

    # ä»¥ fMRI ä½œä¸ºå‚è€ƒæ¨¡æ€ï¼ˆæ—¶é—´æ­¥æœ€å°‘ï¼Œé¿å…åˆ†æ•°çª—å£ï¼‰
    # è‹¥æ—  fMRI åˆ™å–èŠ‚ç‚¹æ•°ç¬¬ä¸€é¡¹
    ref_type = 'fmri' if 'fmri' in node_types else node_types[0]

    # â”€â”€ è·¨æ¨¡æ€æ—¶é—´å¯¹é½ï¼ˆå¯é€‰ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # é»˜è®¤ï¼ˆcross_modal_align=Falseï¼‰ï¼šå„æ¨¡æ€ä½¿ç”¨å„è‡ªçš„è‡ªç„¶æ—¶é—´å°ºåº¦ã€‚
    #   fMRI 50 TRs â‰ˆ 100sï¼ˆæ…¢è¡€åŠ¨åŠ›å­¦ï¼‰ï¼ŒEEG 500 pts = 2sï¼ˆå¿«ç¥ç»æŒ¯è¡ï¼‰ã€‚
    #   é€‚ç”¨äºï¼šå„æ¨¡æ€é¢„æµ‹è‡ªèº«æœªæ¥ï¼ˆintra-modal predictionï¼Œé»˜è®¤åœºæ™¯ï¼‰ã€‚
    #
    # cross_modal_align=Trueï¼šå¼ºåˆ¶æ‰€æœ‰æ¨¡æ€çª—å£è¦†ç›–ç›¸åŒçš„å®é™…æ—¶é•¿ã€‚
    #   ws_eeg = round(ws_fmri Ã— T_eeg / T_fmri)
    #   é€‚ç”¨äºï¼šè·¨æ¨¡æ€é¢„æµ‹ï¼ˆEEGâ†’fMRIã€fMRIâ†’EEGï¼‰ã€‚
    #   âš  æ³¨æ„ï¼šå¯¹é½å EEG çª—å£çº¦ 12500 ptsï¼ˆ500s at 250Hzï¼‰ï¼Œ
    #            å¯èƒ½å¯¼è‡´ CUDA OOMã€‚ç¡®ä¿ VRAM è¶³å¤Ÿåå†å¯ç”¨ã€‚
    if w_cfg.get('cross_modal_align', False) and ref_type == 'fmri' and 'eeg' in node_types:
        T_fmri_ref = T_per_type['fmri']
        if T_fmri_ref > 0:
            T_eeg = T_per_type['eeg']
            ws_fmri = window_sizes['fmri']
            window_sizes['eeg'] = round(ws_fmri * (T_eeg / T_fmri_ref))
            logger.debug(
                f"è·¨æ¨¡æ€æ—¶é—´å¯¹é½å·²å¯ç”¨: EEG çª—å£è°ƒæ•´ä¸º {window_sizes['eeg']} pts"
                f" (ä¸ fMRI {ws_fmri} TRs è¦†ç›–ç›¸åŒå®é™…æ—¶é•¿)"
            )
    ws_ref = window_sizes[ref_type]
    T_ref = T_per_type[ref_type]
    stride = max(1, int(ws_ref * stride_fraction))

    if ws_ref >= T_ref:
        # çª—å£è¦†ç›–å®Œæ•´åºåˆ—ï¼šæ— æ³•å†åˆ†å‰²ï¼Œé€€åŒ–ä¸ºåŸå§‹å•æ ·æœ¬
        logger.debug(
            f"çª—å£å¤§å° ({ref_type}: {ws_ref}) â‰¥ åºåˆ—é•¿åº¦ ({T_ref})ï¼Œ"
            f" çª—å£é‡‡æ ·é€€åŒ–ä¸ºå•æ ·æœ¬ã€‚è‹¥éœ€å¤šçª—å£ï¼Œè¯·å‡å° window_size æˆ–"
            f" å¢å¤§åºåˆ—ï¼ˆè®¾ max_seq_len: nullï¼‰ã€‚"
        )
        return [full_graph]

    window_starts = list(range(0, T_ref - ws_ref + 1, stride))

    windows: List[HeteroData] = []
    for t_start_ref in window_starts:
        win = HeteroData()

        # å…±äº«å›¾æ‹“æ‰‘ï¼ˆæ‰€æœ‰çª—å£ä½¿ç”¨ç›¸åŒçš„ edge_indexï¼Œæ¥è‡ªå…¨åºåˆ—è¿é€šæ€§ä¼°è®¡ï¼‰
        for edge_type in full_graph.edge_types:
            win[edge_type].edge_index = full_graph[edge_type].edge_index
            if hasattr(full_graph[edge_type], 'edge_attr'):
                win[edge_type].edge_attr = full_graph[edge_type].edge_attr

        # æŒ‰æ¯”ä¾‹å¯¹é½å„æ¨¡æ€çš„çª—å£åˆ‡ç‰‡
        for nt in node_types:
            T_nt = T_per_type[nt]
            ws_nt = window_sizes[nt]
            # æ ¹æ®å‚è€ƒæ¨¡æ€æ—¶é—´æ­¥æ¯”ä¾‹ï¼Œç­‰æ¯”ä¾‹å®šä½è¯¥æ¨¡æ€çš„èµ·å§‹ç‚¹
            # ä½¿ç”¨ int() è€Œé round()ï¼šæ•°ç»„ç´¢å¼•ç”¨æ•´æ•°æˆªæ–­è¯­ä¹‰æ›´å¯é¢„æœŸ
            t_start_nt = int(t_start_ref * (T_nt / T_ref))
            t_end_nt = t_start_nt + ws_nt

            x_full = full_graph[nt].x  # [N, T, C]
            if t_end_nt > T_nt:
                # æœ«å°¾çª—å£è¶Šç•Œï¼šç”¨é›¶å¡«å……ä¿æŒå›ºå®š T=ws_nt
                x_slice = x_full[:, t_start_nt:, :]
                pad_len = ws_nt - x_slice.shape[1]
                pad = torch.zeros(
                    x_slice.shape[0], pad_len, x_slice.shape[2],
                    dtype=x_slice.dtype,
                )
                x_slice = torch.cat([x_slice, pad], dim=1)
            else:
                x_slice = x_full[:, t_start_nt:t_end_nt, :]

            win[nt].x = x_slice
            # å¤åˆ¶ã€Œçª—å£é™æ€ã€å±æ€§ï¼šåœ¨åŒä¸€ run çš„æ‰€æœ‰çª—å£é—´ä¸éšæ—¶é—´å˜åŒ–ã€‚
            # - num_nodes: èŠ‚ç‚¹æ•°ï¼Œç”¨äº PyG å†…éƒ¨å›¾æ„å»º
            # - pos: ç©ºé—´åæ ‡ï¼ˆEEG ç”µæåæ ‡ / fMRI ROI è´¨å¿ƒï¼‰ï¼Œç”¨äºå¯è§†åŒ–å’Œè·ç¦»åŠ æƒè¾¹
            # - sampling_rate: é‡‡æ ·ç‡ï¼Œç”¨äºæ—¥å¿—æ‘˜è¦ï¼ˆä¸éšçª—å£å˜åŒ–ï¼‰
            # - labels: ROI/é€šé“åç§°ï¼Œç”¨äºå¯è§£é‡Šæ€§åˆ†æï¼ˆä¾‹å¦‚ visualization.pyï¼‰
            # æœ‰æ„ä¸å¤åˆ¶çš„å±æ€§ï¼ˆéçª—å£é™æ€æˆ–å½“å‰æœªè¯»å–ï¼‰ï¼š
            # - atlas_mapping: EEGâ†’atlas ROI æ˜ å°„ï¼ŒSET åä»æœªè¢« READï¼Œæš‚ä¸ä¼ æ’­
            # - temporal_length: å…¨åºåˆ—é•¿åº¦ï¼Œçª—å£ä¸­åº”ä½¿ç”¨ x.shape[1] åŠ¨æ€è·å–
            for attr in ('num_nodes', 'pos', 'sampling_rate', 'labels'):
                if hasattr(full_graph[nt], attr):
                    setattr(win[nt], attr, getattr(full_graph[nt], attr))

        windows.append(win)

    # å¤åˆ¶å›¾çº§å±æ€§ï¼ˆä¸å±äºä»»ä½•èŠ‚ç‚¹ç±»å‹ï¼Œä½†å¯¹æ•´ä¸ªæ ·æœ¬å…±äº«çš„å…ƒæ•°æ®ï¼‰
    # å½“å‰å›¾çº§å±æ€§ï¼š
    #   subject_idx:    è¢«è¯•æ•´æ•°ç´¢å¼•ï¼Œä¾› subject embeddingï¼ˆä¸ªæ€§åŒ–ï¼‰ä½¿ç”¨ï¼ˆAGENTS.md Â§ä¹ Gap 2ï¼‰
    #   run_idx:        æ‰«æ run çš„å…¨å±€æ•´æ•°ç´¢å¼•ï¼Œä¾› train_model run-level åˆ’åˆ†
    #   task_id:        ä»»åŠ¡åå­—ç¬¦ä¸²ï¼ˆå¦‚ 'GRADON'ï¼‰ï¼Œä¾›æ—¥å¿—æ‘˜è¦æŒ‰ä»»åŠ¡ç»Ÿè®¡
    #   subject_id_str: è¢«è¯• ID å­—ç¬¦ä¸²ï¼ˆå¦‚ 'sub-01'ï¼‰ï¼Œä¾›æ—¥å¿—æ‘˜è¦æŒ‰è¢«è¯•ç»Ÿè®¡
    for attr in ('subject_idx', 'run_idx', 'task_id', 'subject_id_str'):
        if hasattr(full_graph, attr):
            for win in windows:
                setattr(win, attr, getattr(full_graph, attr))

    return windows


def build_graphs(config: dict, logger: logging.Logger):
    """åŠ è½½æ•°æ®å¹¶æ„å»ºå›¾ç»“æ„ï¼ˆå«ç¼“å­˜æ„ŸçŸ¥çš„æŒ‰éœ€æ•°æ®åŠ è½½ï¼‰ã€‚

    å°†åŸå…ˆæ‹†æˆä¸¤æ­¥çš„ã€Œå‡†å¤‡æ•°æ®ã€+ã€Œæ„å»ºå›¾ã€åˆå¹¶ä¸ºä¸€æ­¥ï¼š
    å¯¹æ¯ä¸ª (è¢«è¯•, ä»»åŠ¡) ç»„åˆå…ˆæ£€æŸ¥å›¾ç¼“å­˜ï¼Œå‘½ä¸­åˆ™ç›´æ¥åŠ è½½ç¼“å­˜å›¾ï¼ˆå®Œå…¨è·³è¿‡
    EEG/fMRI åŸå§‹æ•°æ®çš„è¯»å–ä¸é¢„å¤„ç†ï¼‰ï¼Œæœªå‘½ä¸­æ‰è°ƒç”¨ BrainDataLoader åŠ è½½
    åŸå§‹æ•°æ®ã€æ‰§è¡Œé¢„å¤„ç†å¹¶æ„å»ºå›¾åå†™å…¥ç¼“å­˜ã€‚

    è¿™è§£å†³äº†ä¹‹å‰ã€Œå³ä½¿å›¾ç¼“å­˜å·²å­˜åœ¨ï¼Œæ¯æ¬¡è¿è¡Œä»ä¼šä»å¤´é¢„å¤„ç†æ‰€æœ‰åŸå§‹æ•°æ®ã€çš„é—®é¢˜ã€‚
    """
    logger.info("=" * 60)
    logger.info("æ­¥éª¤ 1-2/4: åŠ è½½æ•°æ® & æ„å»ºå›¾ç»“æ„")
    logger.info("=" * 60)

    # â”€â”€ åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    fmri_task_mapping = config['data'].get('fmri_task_mapping') or {}
    if fmri_task_mapping:
        logger.info(f"fMRI ä»»åŠ¡æ˜ å°„: {fmri_task_mapping}")
        logger.info(
            "  è¯´æ˜ï¼šé…ç½®äº†æ˜ å°„åï¼Œä»»åŠ¡å‘ç°ä»…æ‰«æ EEG æ–‡ä»¶ï¼Œ"
            "fMRI æ–‡ä»¶ç”±æ˜ å°„å…³ç³»ç¡®å®šï¼ˆé¿å… fMRI-only ä»»åŠ¡äº§ç”Ÿæ—  EEG çš„å•æ¨¡æ€å›¾ï¼‰ã€‚"
        )
    data_loader = BrainDataLoader(
        data_root=config['data']['root_dir'],
        modalities=config['data']['modalities'],
        fmri_task_mapping=fmri_task_mapping if fmri_task_mapping else None,
    )
    
    _MIN_VOLUMES = 10  # Shared threshold for minimum valid fMRI timepoints

    def process_fmri_timeseries(fmri_data, min_volumes=_MIN_VOLUMES):
        """Extract and normalize fMRI timeseries.

        Handles all common input shapes:
        - 4-D [X, Y, Z, T]: raw volumetric fMRI â†’ averaged to [1, T]
        - 3-D [N_rois, T, ?] or [X, Y, T]: reshaped â†’ averaged to [1, T]
        - 2-D [N_rois, T] or [T, N_rois]: pre-parcellated ROI data â†’ ALL ROIs
          kept as separate nodes [N_rois, T] (no averaging).

        Returns (timeseries [N_rois, T], error_or_None).
        """
        if fmri_data.ndim == 4:
            n_volumes = fmri_data.shape[-1]
            if n_volumes < min_volumes:
                return None, f"Too few volumes: {n_volumes} < {min_volumes}"
            # Average all in-mask voxels â€” single timeseries
            fmri_ts = fmri_data.reshape(-1, n_volumes).mean(axis=0)
            fmri_ts = (fmri_ts - fmri_ts.mean()) / (fmri_ts.std() + 1e-8)
            return fmri_ts.reshape(1, -1), None

        elif fmri_data.ndim == 3:
            n_volumes = fmri_data.shape[-1]
            if n_volumes < min_volumes:
                return None, f"Too few volumes: {n_volumes} < {min_volumes}"
            fmri_ts = fmri_data.reshape(-1, n_volumes).mean(axis=0)
            fmri_ts = (fmri_ts - fmri_ts.mean()) / (fmri_ts.std() + 1e-8)
            return fmri_ts.reshape(1, -1), None

        elif fmri_data.ndim == 2:
            # Already ROI timeseries â€” preserve all ROIs as separate graph nodes.
            # Ensure layout is [N_rois, T].
            if fmri_data.shape[0] > fmri_data.shape[1]:
                fmri_data = fmri_data.T
            N_rois, T = fmri_data.shape
            if T < min_volumes:
                return None, f"Too few timepoints: {T} < {min_volumes}"
            # Normalise each ROI independently
            mean = fmri_data.mean(axis=1, keepdims=True)
            std = fmri_data.std(axis=1, keepdims=True) + 1e-8
            return (fmri_data - mean) / std, None

        else:
            return None, f"Unsupported fMRI shape: {fmri_data.shape}"

    def _parcellate_fmri_with_atlas(fmri_img, atlas_path: Path) -> Optional[np.ndarray]:
        """Apply atlas parcellation to extract per-ROI timeseries.

        Uses nilearn NiftiLabelsMasker which handles resampling automatically.
        Returns [N_rois, T] float32 array, or None on failure.

        Why this matters: without parcellation fMRI is collapsed to a single node
        ([1, T]), making graph convolution meaningless.  With the Schaefer200 atlas
        we get ~190-200 anatomically meaningful nodes â€” the actual design intent.
        The exact count depends on how many atlas parcels overlap with the
        subject's fMRI brain mask after resampling; see logging below.
        """
        # Infer the expected number of parcels from the atlas file name.
        # Pattern: "schaeferNNN" â†’ NNN parcels; "aal116" â†’ 116; etc.
        # Falls back to None (unknown) for non-standard names.
        import re as _re
        _m = _re.search(r'_?(\d{2,4})_?[Pp]arcels', atlas_path.name)
        if _m is None:
            _m = _re.search(r'[Ss]chaefer(\d+)', atlas_path.name)
        expected_n_rois: Optional[int] = int(_m.group(1)) if _m else None
        try:
            try:
                from nilearn.maskers import NiftiLabelsMasker  # nilearn >= 0.10
            except ImportError:
                from nilearn.input_data import NiftiLabelsMasker  # nilearn < 0.10
            masker = NiftiLabelsMasker(
                labels_img=str(atlas_path),
                standardize=True,
                detrend=True,
            )
            roi_ts = masker.fit_transform(fmri_img)  # [T, N_rois]
            n_rois = roi_ts.shape[1]
            # NiftiLabelsMasker excludes atlas parcels that have no valid voxels
            # in the fMRI brain mask after resampling the atlas to the EPI
            # resolution.  For Schaefer200 it is normal to get 190-199 ROIs:
            # the excluded parcels are typically temporal-pole and orbitofrontal
            # regions prone to EPI signal dropout, or very small parcels that
            # lose all voxels when resampled from 1 mm to ~3 mm.
            # This is expected behaviour â€” the actual ROI count varies by subject
            # and acquisition and does NOT indicate a problem.
            if expected_n_rois is not None and n_rois < expected_n_rois:
                logger.info(
                    f"Atlas parcellation ({atlas_path.name}): "
                    f"{n_rois}/{expected_n_rois} ROIs extracted. "
                    f"({expected_n_rois - n_rois} parcels excluded â€” no overlap with "
                    f"subject brain mask after resampling; expected for EPI data.)"
                )
            else:
                logger.info(
                    f"Atlas parcellation ({atlas_path.name}): {n_rois} ROIs extracted."
                    + (" (all parcels have valid voxel coverage)" if expected_n_rois and n_rois == expected_n_rois else "")
                )
            if roi_ts.shape[0] < _MIN_VOLUMES:
                logger.warning(
                    f"Atlas parcellation produced only {roi_ts.shape[0]} timepoints; skipping."
                )
                return None
            if roi_ts.shape[1] < 2:
                logger.warning(
                    f"Atlas parcellation produced only {roi_ts.shape[1]} ROIs; skipping."
                )
                return None
            return roi_ts.T.astype(np.float32)  # [N_rois, T]
        except Exception as e:
            logger.warning(f"Atlas parcellation failed ({e}); falling back to single-node fMRI.")
            return None

    # åˆå§‹åŒ–å›¾æ˜ å°„å™¨
    mapper = GraphNativeBrainMapper(
        atlas_name=config['data']['atlas']['name'],
        add_self_loops=config['graph']['add_self_loops'],
        make_undirected=config['graph']['make_undirected'],
        k_nearest_fmri=config['graph'].get('k_nearest_fmri', 20),
        k_nearest_eeg=config['graph'].get('k_nearest_eeg', 10),
        threshold_fmri=config['graph'].get('threshold_fmri', 0.3),
        threshold_eeg=config['graph'].get('threshold_eeg', 0.2),
        device=config['device']['type'],
        eeg_connectivity_method=config['graph'].get('eeg_connectivity_method', 'correlation'),
    )

    # Resolve atlas file path once (relative to project root)
    atlas_file = Path(__file__).parent / config['data']['atlas']['file']
    if atlas_file.exists():
        logger.info(
            f"Atlas parcellation enabled: {atlas_file.name} â†’ up to 200 fMRI ROI nodes"
        )
    else:
        logger.warning(
            f"Atlas file not found at {atlas_file}; fMRI will use single-node fallback."
        )
    
    # â”€â”€ å›¾ç¼“å­˜è®¾ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    cache_cfg = config['data'].get('cache', {})
    cache_enabled = cache_cfg.get('enabled', False)
    cache_dir: Optional[Path] = None
    if cache_enabled:
        # ç›¸å¯¹è·¯å¾„ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼ˆå³ main.py æ‰€åœ¨ç›®å½•ï¼‰è§£æï¼Œ
        # è€Œéè¿è¡Œæ—¶å·¥ä½œç›®å½•ï¼ˆCWDï¼‰â€”â€”è¿™ä¿è¯äº†æ— è®ºç”¨æˆ·ä»å“ªä¸ªç›®å½•å¯åŠ¨
        # è„šæœ¬ï¼Œç¼“å­˜è·¯å¾„å§‹ç»ˆå›ºå®šä¸€è‡´ï¼Œä¸ä¼šå›  CWD ä¸åŒå¯¼è‡´ã€Œæ‰¾ä¸åˆ°ç¼“å­˜ã€ã€‚
        _cache_dir_cfg = cache_cfg.get('dir', 'outputs/graph_cache')
        _cache_dir_path = Path(_cache_dir_cfg)
        cache_dir = (
            _cache_dir_path
            if _cache_dir_path.is_absolute()
            else Path(__file__).parent / _cache_dir_path
        )
        try:
            cache_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"å›¾ç¼“å­˜å·²å¯ç”¨: {cache_dir}")
        except OSError as e:
            logger.warning(f"æ— æ³•åˆ›å»ºç¼“å­˜ç›®å½• {cache_dir}: {e}ï¼Œç¼“å­˜å·²ç¦ç”¨")
            cache_dir = None

    # â”€â”€ æ—¶é—´çª—å£é‡‡æ ·é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    w_cfg = config.get('windowed_sampling', {})
    windowed = w_cfg.get('enabled', False)

    # å½“æ—¶é—´çª—å£é‡‡æ ·å¼€å¯æ—¶ï¼Œè¿é€šæ€§ç”±å®Œæ•´ run ä¼°è®¡ï¼ˆä¸æˆªæ–­ï¼‰ï¼›
    # æˆªæ–­ä»…åœ¨å•æ ·æœ¬è®­ç»ƒæ¨¡å¼ï¼ˆwindowed=Falseï¼‰ä¸‹ä¿ç•™ï¼Œç”¨äºé˜² OOMã€‚
    max_seq_len = config['training'].get('max_seq_len', None)
    if windowed:
        if max_seq_len is not None:
            logger.info(
                f"æ—¶é—´çª—å£é‡‡æ ·å·²å¯ç”¨ (fMRI_ws={w_cfg.get('fmri_window_size', 50)}, "
                f"EEG_ws={w_cfg.get('eeg_window_size', 500)}, "
                f"stride={w_cfg.get('stride_fraction', 0.5)}Ã—ws)ã€‚"
                f" å›¾æ„å»ºå°†ä½¿ç”¨å®Œæ•´åºåˆ—ä»¥è·å¾—å¯é è¿é€šæ€§ä¼°è®¡"
                f"ï¼ˆmax_seq_len={max_seq_len} ä»…åœ¨å•æ ·æœ¬æ¨¡å¼ä¸‹ç”Ÿæ•ˆï¼‰ã€‚"
                f" å»ºè®®è®¾ max_seq_len: null ä»¥å®Œå…¨åˆ©ç”¨å…¨åºåˆ—ã€‚"
            )
        else:
            logger.info(
                f"æ—¶é—´çª—å£é‡‡æ ·å·²å¯ç”¨ (fMRI_ws={w_cfg.get('fmri_window_size', 50)}, "
                f"EEG_ws={w_cfg.get('eeg_window_size', 500)}, "
                f"stride={w_cfg.get('stride_fraction', 0.5)}Ã—ws)ã€‚"
                f" å›¾æ„å»ºå°†ä½¿ç”¨å®Œæ•´åºåˆ—ã€‚"
            )
    else:
        if max_seq_len is not None:
            logger.info(f"åºåˆ—æˆªæ–­å·²å¯ç”¨: max_seq_len={max_seq_len} (é˜²æ­¢ CUDA OOM)")

    # â”€â”€ è¢«è¯•/ä»»åŠ¡å‘ç°ï¼ˆçº¯æ–‡ä»¶æ‰«æï¼Œæ— æ•°æ®åŠ è½½ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # åœ¨ä»»ä½•æ•°æ®åŠ è½½ä¹‹å‰å…ˆæšä¸¾æ‰€æœ‰ (è¢«è¯•, ä»»åŠ¡) å¯¹ï¼Œç”¨äºï¼š
    # 1. å»ºç«‹å…¨å±€ subject_to_idxï¼ˆsubject embedding ç´¢å¼•å¿…é¡»åœ¨å…¨æ•°æ®é›†ä¸­å”¯ä¸€ä¸”ç¨³å®šï¼‰
    # 2. é€å¯¹æ£€æŸ¥ç¼“å­˜ï¼Œå‘½ä¸­åˆ™è·³è¿‡åŸå§‹æ•°æ®åŠ è½½ï¼Œå¤§å¹…ç¼©çŸ­äºŒæ¬¡è¿è¡Œå¯åŠ¨æ—¶é—´
    tasks_cfg = config['data'].get('tasks')
    if tasks_cfg is None:
        legacy_task = config['data'].get('task')
        if legacy_task is not None:
            tasks_cfg = [legacy_task]
            logger.info(
                f"ä½¿ç”¨æ—§ç‰ˆ 'task: {legacy_task}' é…ç½®ã€‚"
                f" å»ºè®®è¿ç§»åˆ° 'tasks: [{legacy_task}]'ã€‚"
            )
        # tasks_cfg ä»ä¸º None â†’ è‡ªåŠ¨å‘ç°æ‰€æœ‰ä»»åŠ¡
    elif isinstance(tasks_cfg, str):
        tasks_cfg = [tasks_cfg]

    if tasks_cfg is None:
        logger.info("tasks: null â†’ è‡ªåŠ¨å‘ç°æ¯ä¸ªè¢«è¯•çš„æ‰€æœ‰ä»»åŠ¡")
    else:
        logger.info(f"å°†åŠ è½½ä»¥ä¸‹ä»»åŠ¡: {tasks_cfg}")

    # 0 å’Œ null å‡è¡¨ç¤º"ä¸é™åˆ¶è¢«è¯•æ•°"ï¼ˆä¸ load_all_subjects è¡Œä¸ºä¸€è‡´ï¼‰ã€‚
    # ä½¿ç”¨æ˜¾å¼ None æ£€æŸ¥è€Œé or Noneï¼Œé¿å…æ„å¤–å°†åˆæ³•çš„ 0 è§£é‡Šä¸º"ä¸é™åˆ¶"çš„è¯­ä¹‰æ··æ·†ã€‚
    _max_sub = config['data'].get('max_subjects')
    max_subjects_cfg = int(_max_sub) if _max_sub else None  # 0 / null â†’ None

    subject_dirs = sorted(data_loader.data_root.glob("sub-*"))
    if max_subjects_cfg:
        subject_dirs = subject_dirs[:max_subjects_cfg]

    # ç¬¬ä¸€éï¼šä»…æ‰«ææ–‡ä»¶åï¼Œä¸åŠ è½½ä»»ä½•åŸå§‹æ•°æ®
    all_pairs: List[tuple] = []  # [(subject_id, task), ...]
    for subject_dir in subject_dirs:
        subject_id = subject_dir.name
        if tasks_cfg is None:
            subject_tasks = data_loader._discover_tasks(subject_id)
        elif len(tasks_cfg) == 0:
            # ç©ºåˆ—è¡¨ = ä¸è¿‡æ»¤ä»»åŠ¡ï¼Œç›´æ¥åŠ è½½é¦–ä¸ªåŒ¹é…æ–‡ä»¶ï¼ˆä¸ load_all_subjects è¡Œä¸ºä¸€è‡´ï¼‰
            subject_tasks = [None]
        else:
            subject_tasks = list(tasks_cfg)
        for t in subject_tasks:
            all_pairs.append((subject_id, t))

    if not all_pairs:
        raise ValueError(
            "æœªå‘ç°ä»»ä½•è¢«è¯•-ä»»åŠ¡ç»„åˆï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„é…ç½®ã€‚"
            f" data.root_dir={config['data']['root_dir']!r}"
        )

    logger.info(f"å‘ç° {len(all_pairs)} ä¸ªè¢«è¯•-ä»»åŠ¡ç»„åˆ")

    # è¢«è¯• ID â†’ æ•´æ•°ç´¢å¼•æ˜ å°„ï¼ˆç”¨äº subject embeddingï¼ŒAGENTS.md Â§ä¹ Gap 2ï¼‰
    # ç”±æ–‡ä»¶ç³»ç»Ÿå‘ç°ç»“æœç¡®å®šæ€§æ¨å¯¼ï¼Œä¸æ•°æ®åŠ è½½é¡ºåºæ— å…³ï¼Œä¿è¯å…¨å±€å”¯ä¸€ã€‚
    all_subject_ids = sorted(set(sid for sid, _ in all_pairs))
    subject_to_idx = {sid: i for i, sid in enumerate(all_subject_ids)}
    logger.info(f"è¢«è¯•ç´¢å¼•æ˜ å°„å·²å»ºç«‹: {len(subject_to_idx)} ä¸ªè¢«è¯•")

    graphs: List[HeteroData] = []
    n_cached = 0
    n_windows_total = 0
    run_idx_counter = 0  # æ¯ä¸ª (subject, task) run çš„å…¨å±€æ•´æ•°ç´¢å¼•ï¼Œç”¨äº run-level è®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†
    for subject_id, task in all_pairs:
        # è®¡ç®—ä¸€æ¬¡ç¼“å­˜ keyï¼Œä¾›æœ¬æ¬¡è¿­ä»£çš„"è¯»"å’Œ"å†™"å…±ç”¨ï¼Œé¿å…é‡å¤è®¡ç®—ã€‚
        cache_key = _graph_cache_key(subject_id, task, config) if cache_dir is not None else None

        # â”€â”€ å°è¯•ä»ç¼“å­˜åŠ è½½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ç¼“å­˜å‘½ä¸­æ—¶ç›´æ¥åŠ è½½ .pt æ–‡ä»¶ï¼Œå®Œå…¨è·³è¿‡ EEG/fMRI åŸå§‹æ•°æ®çš„è¯»å–ä¸é¢„å¤„ç†ã€‚
        # è¿™æ˜¯ã€Œç¼“å­˜æ„ŸçŸ¥åŠ è½½ã€çš„æ ¸å¿ƒï¼šåŸå§‹æ•°æ®ä»…åœ¨ç¼“å­˜ç¼ºå¤±æ—¶æ‰ä¼šè¢«è¯»å–ã€‚
        if cache_dir is not None and cache_key is not None:
            cache_path = cache_dir / cache_key
            if cache_path.exists():
                try:
                    full_graph = torch.load(cache_path, map_location='cpu', weights_only=False)
                    # è¢«è¯•ç´¢å¼•ï¼ˆAGENTS.md Â§ä¹ Gap 2ï¼‰ï¼š
                    # ç¼“å­˜å›¾å¯èƒ½ç”± V5.18 æˆ–æ›´æ—©ç‰ˆæœ¬ä¿å­˜ï¼Œä¸å« subject_idxã€‚
                    # ä½¿ç”¨å½“å‰è¿è¡Œæ—¶çš„ subject_to_idx è¡¥å†™ï¼Œä¿è¯åµŒå…¥å¯¹ç¼“å­˜å›¾åŒæ ·ç”Ÿæ•ˆã€‚
                    # å³ä½¿å›¾ç”± V5.19 ä¿å­˜ï¼ˆå·²å« subject_idxï¼‰ï¼Œæ­¤å¤„è¦†å†™ä¹Ÿæ— å®³â€”â€”
                    # subject_to_idx ç”±å½“å‰æ•°æ®é›†çš„ subject_id ç¡®å®šæ€§æ¨å¯¼ï¼Œä¸ä¿å­˜æ—¶ä¸€è‡´ã€‚
                    if subject_id not in subject_to_idx:
                        logger.warning(
                            f"subject_id='{subject_id}' not found in subject_to_idx "
                            f"(cache load path). Falling back to index 0."
                        )
                    full_graph.subject_idx = torch.tensor(
                        subject_to_idx.get(subject_id, 0), dtype=torch.long
                    )
                    # run_idx ä¾› run-level è®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†ä½¿ç”¨ã€‚
                    # ä¸ subject_idx åŒç†ï¼šç¼“å­˜å›¾å¯èƒ½ä¸å«æ­¤å±æ€§ï¼Œæ­¤å¤„è¡¥å†™æ— å®³ã€‚
                    full_graph.run_idx = torch.tensor(run_idx_counter, dtype=torch.long)
                    # task_id / subject_id_strï¼šè¡¥å†™ä»»åŠ¡å’Œè¢«è¯•å­—ç¬¦ä¸²å…ƒæ•°æ®ï¼Œ
                    # ä¾› log_training_summary å±•ç¤ºæ•°æ®ç»„æˆã€‚
                    full_graph.task_id = task or ''
                    full_graph.subject_id_str = subject_id
                    win_samples = extract_windowed_samples(full_graph, w_cfg, logger)
                    graphs.extend(win_samples)
                    n_windows_total += len(win_samples)
                    n_cached += 1
                    run_idx_counter += 1
                    logger.debug(
                        f"ä»ç¼“å­˜åŠ è½½å›¾: {cache_key}"
                        + (f" â†’ {len(win_samples)} ä¸ªçª—å£" if windowed else "")
                    )
                    continue
                except Exception as e:
                    logger.warning(f"ç¼“å­˜åŠ è½½å¤±è´¥ ({cache_key}): {e}ï¼Œé‡æ–°æ„å»º")

        # â”€â”€ ç¼“å­˜æœªå‘½ä¸­ï¼šåŠ è½½åŸå§‹æ•°æ®å¹¶æ„å»ºå›¾ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # åªæœ‰åœ¨ç¼“å­˜ä¸­æ‰¾ä¸åˆ°é¢„æ„å»ºå›¾æ—¶ï¼Œæ‰æ‰§è¡Œè€—æ—¶çš„ EEG/fMRI åŸå§‹æ•°æ®åŠ è½½ä¸é¢„å¤„ç†ã€‚
        logger.debug(f"ç¼“å­˜æœªå‘½ä¸­ï¼ŒåŠ è½½åŸå§‹æ•°æ®: {subject_id}/{task}")
        subject_data = data_loader.load_subject(subject_id, task)
        if not subject_data:
            logger.warning(f"è·³è¿‡è¢«è¯• {subject_id}/{task}: æ•°æ®åŠ è½½å¤±è´¥")
            continue
        # ç¡®ä¿ task å­—æ®µä¸€è‡´ï¼šload_subject() ä¸è®¾ç½® 'task'ï¼Œæ­¤å¤„è¡¥å†™ã€‚
        # ï¼ˆæ—§ç‰ˆ load_all_subjects åœ¨ subject_data åŠ å…¥ all_data å‰æ‰æ·»åŠ  task å­—æ®µï¼Œ
        #  æ­¤å¤„ç»´æŒç›¸åŒè¯­ä¹‰ã€‚ï¼‰
        subject_data['task'] = task
        if 'fmri' not in subject_data and 'eeg' not in subject_data:
            logger.warning(f"è·³è¿‡è¢«è¯• {subject_id}/{task}: æ— æœ‰æ•ˆæ¨¡æ€æ•°æ®")
            continue

        graph_list = []

        # fMRIå›¾
        if 'fmri' in subject_data:
            fmri_data = subject_data['fmri']['data']
            fmri_img = subject_data['fmri'].get('img')   # preprocessed NIfTI object

            # Prefer atlas parcellation: gives [N_rois, T] (e.g. 200 ROI nodes).
            # Fallback to spatial average â†’ [1, T] when atlas unavailable.
            fmri_ts = None
            if atlas_file.exists() and fmri_img is not None:
                fmri_ts = _parcellate_fmri_with_atlas(fmri_img, atlas_file)

            if fmri_ts is None:
                fmri_ts, error = process_fmri_timeseries(fmri_data)
                if error:
                    logger.warning(f"fMRI processing failed: {error}, skipping subject")
                    continue
            
            # â”€â”€ æ¡ä»¶ç‰¹å¼‚æ€§æ—¶é—´æ®µæˆªå– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # é€‚ç”¨åœºæ™¯ï¼šå¤šä¸ª EEG æ¡ä»¶ï¼ˆGRADON/GRADOFFï¼‰å…±äº«åŒä¸€ fMRI è¿è¡Œï¼ˆå¦‚ task-CBï¼‰ï¼Œ
            # fMRI æ–‡ä»¶æŒ‰æ¡ä»¶é¡ºåºåŒ…å«å®Œæ•´ä¼šè¯ï¼Œä¾‹å¦‚ï¼š
            #   t=0..150  TRs â†’ GRADON æ¡ä»¶
            #   t=150..300 TRs â†’ GRADOFF æ¡ä»¶
            #
            # è‹¥ä¸æˆªå–ï¼Œä¼šäº§ç”Ÿä¸¤ç±»é”™è¯¯ï¼š
            #   1. éçª—å£æ¨¡å¼ï¼šä¸¤ä¸ªæ¡ä»¶çš„ max_seq_len æˆªæ–­éƒ½ä» t=0 å¼€å§‹ï¼Œ
            #      GRADOFF é”™è¯¯åœ°ä½¿ç”¨ GRADON æ—¶æ®µçš„ fMRI æ•°æ®ã€‚
            #   2. çª—å£æ¨¡å¼ï¼šæ»‘åŠ¨çª—å£è·¨è¶Šæ¡ä»¶è¾¹ç•Œï¼ˆå¦‚ t=275-325 åŒæ—¶åŒ…å«
            #      GRADON æœ«å°¾å’Œ GRADOFF å¼€å¤´ï¼‰ï¼Œé¢„æµ‹æŸå¤±åœ¨è¾¹ç•Œå¤„å­¦ä¹ 
            #      è™šå‡çš„æ—¶åºæ¼”åŒ–ï¼Œæ¨¡å‹è¢«è¿«"é¢„æµ‹"å®éªŒæ¡ä»¶åˆ‡æ¢è€Œéç¥ç»åŠ¨æ€ã€‚
            #
            # é…ç½®æ–¹å¼ï¼ˆå•ä½ï¼šTRï¼‰ï¼š
            #   fmri_condition_bounds:
            #     GRADON: [0, 150]
            #     GRADOFF: [150, 300]
            #
            # åº”ç”¨æ—¶æœºï¼šåœ¨è¿é€šæ€§ä¼°è®¡ (mapper.map_fmri_to_graph) ä¹‹å‰æˆªå–ï¼Œ
            # ä¿è¯æ¡ä»¶ç‰¹å¼‚æ€§è¿é€šæ€§çŸ©é˜µå’Œçª—å£å‡ä¸è·¨è¶Šæ¡ä»¶è¾¹ç•Œã€‚
            condition_bounds_cfg = config['data'].get('fmri_condition_bounds') or {}
            if task in condition_bounds_cfg:
                bounds = condition_bounds_cfg[task]
                # Validate format early: must be a sequence of at least 2 numbers.
                if not (hasattr(bounds, '__len__') and len(bounds) >= 2):
                    raise ValueError(
                        f"fmri_condition_bounds['{task}'] must be [start_TR, end_TR], "
                        f"got: {bounds!r}. Example: {{'GRADON': [0, 150]}}"
                    )
                start_tr, end_tr = int(bounds[0]), int(bounds[1])
                T_full = fmri_ts.shape[1]
                if end_tr > T_full:
                    logger.warning(
                        f"fMRI æ¡ä»¶æ—¶é—´æ®µè¶Šç•Œ: task={task}, "
                        f"é…ç½® end_tr={end_tr} > T_full={T_full}ã€‚"
                        f" å°†æˆªæ–­è‡³ {T_full}ã€‚"
                    )
                    end_tr = T_full
                if start_tr >= end_tr:
                    logger.warning(
                        f"fMRI æ¡ä»¶æ—¶é—´æ®µæ— æ•ˆ: task={task}, [{start_tr}, {end_tr}) ä¸ºç©ºåŒºé—´ï¼Œ"
                        f"è·³è¿‡è¾¹ç•Œæˆªå–ï¼ˆå¯èƒ½å¯¼è‡´è·¨æ¡ä»¶æ—¶åºå­¦ä¹ ï¼‰ã€‚"
                    )
                else:
                    fmri_ts = fmri_ts[:, start_tr:end_tr]
                    logger.info(
                        f"fMRI æ¡ä»¶æ—¶é—´æ®µæˆªå–: task={task}, "
                        f"TRs [{start_tr}, {end_tr}) â†’ {fmri_ts.shape[1]} TPs"
                        f"ï¼ˆé˜²æ­¢è·¨æ¡ä»¶è¾¹ç•Œçš„æ—¶åºå­¦ä¹ ï¼‰"
                    )

            # æˆªæ–­ä»…åœ¨å•æ ·æœ¬è®­ç»ƒæ¨¡å¼ä¸‹å¯ç”¨ï¼ˆé˜² CUDA OOMï¼‰ã€‚
            # çª—å£æ¨¡å¼ä¸‹ä¸æˆªæ–­ï¼Œä»¥ä½¿è¿é€šæ€§ä¼°è®¡æ¥è‡ªå®Œæ•´ï¼ˆæ¡ä»¶ç‰¹å¼‚æ€§ï¼‰åºåˆ—ã€‚
            if not windowed and max_seq_len is not None:
                fmri_ts = truncate_timeseries(fmri_ts, max_seq_len)
            
            logger.debug(f"fMRI timeseries shape: {fmri_ts.shape} â†’ {fmri_ts.shape[0]} nodes")
            fmri_graph = mapper.map_fmri_to_graph(
                timeseries=fmri_ts,
                connectivity_matrix=None,  # è‡ªåŠ¨è®¡ç®—
            )
            graph_list.append(('fmri', fmri_graph))
        
        # EEGå›¾
        if 'eeg' in subject_data:
            eeg_data = subject_data['eeg']['data']  # [n_channels, n_times]
            eeg_ch_names = subject_data['eeg']['ch_names']
            # ä» loader è·å–çœŸå®é‡‡æ ·ç‡å’Œç”µæåæ ‡ï¼ˆç”± EEGPreprocessor é€šè¿‡
            # standard_1020 montage è®¾ç½®ï¼Œå•ä½ mmï¼‰
            eeg_sfreq = subject_data['eeg'].get('sfreq', 250.0)
            eeg_ch_pos = subject_data['eeg'].get('ch_pos', None)  # [N_ch, 3] mm æˆ– None
            
            # Validate EEG data
            if eeg_data.shape[0] < 8:
                logger.warning(f"EEG has too few channels: {eeg_data.shape[0]}, skipping")
                continue
            if eeg_data.shape[1] < 100:
                logger.warning(f"EEG has too few timepoints: {eeg_data.shape[1]}, skipping")
                continue
            if np.isnan(eeg_data).any() or np.isinf(eeg_data).any():
                logger.warning("EEG contains NaN or Inf values, skipping")
                continue
            
            # æˆªæ–­ä»…åœ¨å•æ ·æœ¬è®­ç»ƒæ¨¡å¼ä¸‹å¯ç”¨ï¼ˆé˜² CUDA OOMï¼‰ã€‚
            # çª—å£æ¨¡å¼ä¸‹ä¸æˆªæ–­ï¼Œä»¥ä½¿è¿é€šæ€§ä¼°è®¡æ¥è‡ªå®Œæ•´ runã€‚
            if not windowed and max_seq_len is not None:
                eeg_data = truncate_timeseries(eeg_data, max_seq_len)
            
            eeg_graph = mapper.map_eeg_to_graph(
                timeseries=eeg_data,
                channel_names=eeg_ch_names,
                channel_positions=eeg_ch_pos,
                sampling_rate=eeg_sfreq,
            )
            graph_list.append(('eeg', eeg_graph))
        
        # åˆå¹¶å›¾ - FIX: Properly merge multi-modal graphs
        if len(graph_list) > 0:
            if len(graph_list) == 1:
                # Single modality: use as-is
                built_graph = graph_list[0][1]
            else:
                # Multi-modal: merge into heterograph
                built_graph = HeteroData()
                for modality, graph in graph_list:
                    # Copy node features, structure, AND metadata
                    for key in graph.node_types:
                        built_graph[key].x = graph[key].x
                        if hasattr(graph[key], 'num_nodes'):
                            built_graph[key].num_nodes = graph[key].num_nodes
                        if hasattr(graph[key], 'pos'):
                            built_graph[key].pos = graph[key].pos
                        # sampling_rate used by log_training_summary; omitting it
                        # causes silent fallback to wrong defaults (250 Hz / 0.5 Hz)
                        if hasattr(graph[key], 'sampling_rate'):
                            built_graph[key].sampling_rate = graph[key].sampling_rate
                    
                    # Copy edge structure
                    for edge_type in graph.edge_types:
                        built_graph[edge_type].edge_index = graph[edge_type].edge_index
                        if hasattr(graph[edge_type], 'edge_attr'):
                            built_graph[edge_type].edge_attr = graph[edge_type].edge_attr
                
                # è·¨æ¨¡æ€è¾¹ï¼šEEG â†’ fMRI
                # è®¾è®¡ç†å¿µï¼šEEG ç”µæï¼ˆè¾ƒå°‘èŠ‚ç‚¹ï¼‰å‘ fMRI ROIï¼ˆè¾ƒå¤šèŠ‚ç‚¹ï¼‰æŠ•å°„ä¿¡å·ã€‚
                # create_simple_cross_modal_edges è¿”å› (edge_index, edge_attr)ï¼Œ
                # å…¶ä¸­ edge_attr ä¸ºå‡åŒ€æƒé‡ï¼ˆ1.0ï¼‰ï¼Œä¿æŒä¸åŒæ¨¡æ€è¾¹ä¸€è‡´çš„åŠ æƒè¯­ä¹‰ã€‚
                if 'fmri' in built_graph.node_types and 'eeg' in built_graph.node_types:
                    cross_result = mapper.create_simple_cross_modal_edges(built_graph)
                    if cross_result is not None:
                        cross_edges, cross_weights = cross_result
                        built_graph['eeg', 'projects_to', 'fmri'].edge_index = cross_edges
                        built_graph['eeg', 'projects_to', 'fmri'].edge_attr = cross_weights

            # DTI ç»“æ„è¿é€šæ€§è¾¹ï¼ˆå¯é€‰ï¼‰
            # å½“è¢«è¯•æœ‰é¢„è®¡ç®— DTI è¿é€šæ€§çŸ©é˜µä¸”é…ç½®å¯ç”¨æ—¶ï¼Œ
            # åœ¨ fMRI èŠ‚ç‚¹ä¸Šæ–°å¢ ('fmri','structural','fmri') è¾¹ç±»å‹ã€‚
            # æ­¤ä¸¾ä½¿ç¼–ç å™¨èƒ½åŒæ—¶åˆ©ç”¨ fMRI åŠŸèƒ½è¿é€šæ€§ï¼ˆç›¸å…³æ€§è¾¹ï¼‰
            # å’Œ DTI ç»“æ„è¿é€šæ€§ï¼ˆç™½è´¨çº¤ç»´è¾¹ï¼‰ï¼Œä½“ç°å¼‚è´¨å›¾"å¤šè¾¹ç±»å‹"çš„æ ¸å¿ƒä»·å€¼ã€‚
            dti_cfg = config['data'].get('dti_structural_edges', False)
            dti_subject = subject_data.get('dti')
            if dti_cfg and dti_subject is not None and 'fmri' in built_graph.node_types:
                mapper.add_dti_structural_edges(
                    built_graph,
                    dti_subject['connectivity'],
                )

            # è¢«è¯•ç´¢å¼•ï¼ˆAGENTS.md Â§ä¹ Gap 2ï¼šä¸ªæ€§åŒ– subject embeddingï¼‰
            # graph-level å±æ€§ï¼šæ¯ä¸ªå›¾ï¼ˆå…¨åºåˆ—å›¾å’Œçª—å£æ ·æœ¬ï¼‰å‡æºå¸¦ï¼Œ
            # ä¾› GraphNativeBrainModel.forward() æŸ¥è¯¢ subject embeddingã€‚
            # subject_id å¿…é¡»åœ¨ subject_to_idx ä¸­ï¼ˆä¸Šæ–¹é¢„æ‰«æä¿è¯ï¼‰ï¼›
            # è‹¥ KeyError å‘ç”Ÿï¼Œè¯´æ˜ subject_id åœ¨é¢„æ‰«æå’Œæ­¤å¤„ä½¿ç”¨ä¹‹é—´ä¸ä¸€è‡´ï¼ˆé€»è¾‘ bugï¼‰ã€‚
            if subject_id not in subject_to_idx:
                logger.warning(
                    f"subject_id='{subject_id}' not found in subject_to_idx. "
                    f"This indicates a logic bug â€” subject_id changed between pre-scan and use. "
                    f"Falling back to index 0; this subject's embedding will be shared with another."
                )
            built_graph.subject_idx = torch.tensor(
                subject_to_idx.get(subject_id, 0), dtype=torch.long
            )
            # run_idx: å…¨å±€ run é¡ºåºç´¢å¼•ï¼Œä¾› run-level è®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†ã€‚
            # çª—å£æ¨¡å¼ä¸‹ï¼ŒåŒä¸€ run çš„æ‰€æœ‰çª—å£å…±äº«ç›¸åŒ run_idxï¼Œ
            # è®­ç»ƒæ—¶ä»¥ run ä¸ºå•ä½åˆ’åˆ†ï¼Œé˜²æ­¢é‡å çª—å£åŒæ—¶å‡ºç°åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­ã€‚
            built_graph.run_idx = torch.tensor(run_idx_counter, dtype=torch.long)
            # task_id: ä»»åŠ¡åç§°å­—ç¬¦ä¸²ï¼Œä¾›æ—¥å¿—æ‘˜è¦å’Œæ¯ä»»åŠ¡ç»Ÿè®¡ä½¿ç”¨ã€‚
            # æ³¨æ„ï¼šæ­¤å¤„å­˜å‚¨å­—ç¬¦ä¸²è€Œéæ•´æ•°ï¼Œä»…ç”¨äºæ—¥å¿—å’Œè°ƒè¯•ï¼Œä¸å‚ä¸æ¨¡å‹è®¡ç®—ã€‚
            built_graph.task_id = task or ''
            # subject_id_str: è¢«è¯• ID å­—ç¬¦ä¸²ï¼ˆä¸ subject_idx æ•´æ•°ä¸€ä¸€å¯¹åº”ï¼‰ã€‚
            # å•ç‹¬ä¿ç•™å­—ç¬¦ä¸²å½¢å¼ï¼Œä¾›æ—¥å¿—æ‘˜è¦æŒ‰è¢«è¯•ç»Ÿè®¡æ ·æœ¬æ•°æ—¶ä½¿ç”¨ã€‚
            built_graph.subject_id_str = subject_id

            # â”€â”€ ä¿å­˜åˆ°ç¼“å­˜ï¼ˆå§‹ç»ˆä¿å­˜å®Œæ•´ run å›¾ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if cache_dir is not None and cache_key is not None:
                try:
                    cache_path = cache_dir / cache_key
                    torch.save(built_graph, cache_path)
                    logger.debug(f"å›¾å·²ç¼“å­˜: {cache_key}")
                except Exception as e:
                    logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥ ({subject_id}/{task}): {e}")

            # â”€â”€ åŠ å…¥è®­ç»ƒåˆ—è¡¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # çª—å£æ¨¡å¼ï¼šåˆ‡åˆ†ä¸ºå¤šä¸ªçŸ­çª—å£æ ·æœ¬ï¼›å•æ ·æœ¬æ¨¡å¼ï¼šç›´æ¥åŠ å…¥å®Œæ•´å›¾ã€‚
            if windowed:
                win_samples = extract_windowed_samples(built_graph, w_cfg, logger)
                graphs.extend(win_samples)
                n_windows_total += len(win_samples)
                logger.debug(
                    f"  {subject_id}/{task}: {len(win_samples)} ä¸ªæ—¶é—´çª—å£æ ·æœ¬"
                )
            else:
                graphs.append(built_graph)
            run_idx_counter += 1

    if len(graphs) == 0:
        raise ValueError("No valid graphs constructed. Check data quality and preprocessing.")

    # â”€â”€ æ±‡æ€»æ—¥å¿— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    n_runs = len(all_pairs)
    if windowed:
        avg_win = n_windows_total / max(n_runs, 1)
        logger.info(
            f"å›¾æ„å»ºå®Œæˆ: {n_runs} æ¡ run â†’ {n_windows_total} ä¸ªæ—¶é—´çª—å£è®­ç»ƒæ ·æœ¬"
            f" (å¹³å‡ {avg_win:.1f} ä¸ªçª—å£/run)"
            + (f"ï¼Œå…¶ä¸­ {n_cached} æ¡ run æ¥è‡ªç¼“å­˜" if n_cached else "")
        )
    elif n_cached > 0:
        logger.info(
            f"å›¾æ„å»ºå®Œæˆ: {len(graphs)} ä¸ªå›¾"
            f" (å…¶ä¸­ {n_cached} ä¸ªæ¥è‡ªç¼“å­˜ï¼Œ{len(graphs) - n_cached} ä¸ªæ–°å»º)"
        )
    else:
        logger.info(f"æˆåŠŸæ„å»º {len(graphs)} ä¸ªå›¾")

    return graphs, mapper, subject_to_idx


def create_model(config: dict, logger: logging.Logger, num_subjects: int = 0):
    """åˆ›å»ºæ¨¡å‹

    Args:
        config: é…ç½®å­—å…¸
        logger: æ—¥å¿—è®°å½•å™¨
        num_subjects: æ•°æ®é›†ä¸­çš„æ€»è¢«è¯•æ•°ï¼ˆç”± build_graphs è¿”å›çš„ subject_to_idx æ¨å¯¼ï¼‰ã€‚
            > 0 æ—¶åœ¨æ¨¡å‹ä¸­åˆ›å»º nn.Embedding(num_subjects, hidden_channels) å®ç°
            ä¸ªæ€§åŒ–è¢«è¯•åµŒå…¥ï¼ˆAGENTS.md Â§ä¹ Gap 2ï¼‰ã€‚
            0 = ç¦ç”¨ï¼ˆé»˜è®¤ï¼Œå…¼å®¹æ—§è¡Œä¸ºï¼‰ã€‚
    """
    logger.info("=" * 60)
    logger.info("æ­¥éª¤ 3/4: åˆ›å»ºæ¨¡å‹")
    logger.info("=" * 60)
    
    # ç¡®å®šèŠ‚ç‚¹å’Œè¾¹ç±»å‹
    node_types = config['data']['modalities']
    edge_types = []

    for modality in node_types:
        edge_types.append((modality, 'connects', modality))

    # è·¨æ¨¡æ€è¾¹ï¼šè®¾è®¡ç†å¿µæ˜¯ EEG â†’ fMRI
    # EEG ç”µæï¼ˆé€šå¸¸ 32â€“64 é€šé“ï¼‰èŠ‚ç‚¹æ•° < fMRI ROIï¼ˆå¦‚ Schaefer200 çš„ 200 ä¸ªï¼‰ï¼Œ
    # å› æ­¤ç”± EEG å‘ fMRI æŠ•å°„æ¶ˆæ¯ç¬¦åˆ"å°‘èŠ‚ç‚¹å‘å¤šèŠ‚ç‚¹ä¼ æ’­"çš„å›¾å·ç§¯è¯­ä¹‰ã€‚
    # ä½¿ç”¨æ¨¡æ€åè€Œéä½ç½®ç´¢å¼•ï¼Œä¿è¯ä¸å— config['data']['modalities'] é¡ºåºå½±å“ã€‚
    if 'eeg' in node_types and 'fmri' in node_types:
        edge_types.append(('eeg', 'projects_to', 'fmri'))
    elif len(node_types) > 1:
        # é EEG/fMRI æ¨¡æ€ç»„åˆçš„é€šç”¨å›é€€
        edge_types.append((node_types[0], 'projects_to', node_types[1]))

    # DTI ç»“æ„è¿é€šæ€§è¾¹ï¼ˆå¯é€‰ï¼‰
    # å½“ dti_structural_edges: true æ—¶ï¼Œç¼–ç å™¨é¢„å…ˆæ³¨å†Œ ('fmri','structural','fmri')
    # è¾¹ç±»å‹ã€‚è‹¥æŸä¸ªå›¾å®é™…ä¸å«è¯¥è¾¹ï¼ˆDTI æ–‡ä»¶ç¼ºå¤±ï¼‰ï¼Œç¼–ç å™¨ä¼šå®‰é™è·³è¿‡
    # ï¼ˆGraphNativeEncoder.forward ä¸­ `if edge_type in edge_index_dict` ä¿æŠ¤ï¼‰ã€‚
    # è¿™æ˜¯å¼‚è´¨å›¾æ‰©å±•æ€§çš„æ ¸å¿ƒä½“ç°ï¼šæ¨¡å‹çŸ¥é“è¯¥è¾¹ç±»å‹ï¼Œä½†ä¸å¼ºä¾èµ–å…¶å­˜åœ¨ã€‚
    if config['data'].get('dti_structural_edges', False) and 'fmri' in node_types:
        edge_types.append(('fmri', 'structural', 'fmri'))
        logger.info("DTIç»“æ„è¿é€šæ€§è¾¹å·²å¯ç”¨: ('fmri','structural','fmri') å·²åŠ å…¥è¾¹ç±»å‹åˆ—è¡¨")

    # è¾“å…¥é€šé“
    in_channels_dict = {modality: 1 for modality in node_types}
    
    # åˆ›å»ºæ¨¡å‹
    model = GraphNativeBrainModel(
        node_types=node_types,
        edge_types=edge_types,
        in_channels_dict=in_channels_dict,
        hidden_channels=config['model']['hidden_channels'],
        num_encoder_layers=config['model']['num_encoder_layers'],
        num_decoder_layers=config['model']['num_decoder_layers'],
        use_prediction=config['model']['use_prediction'],
        prediction_steps=config['model']['prediction_steps'],
        dropout=config['model']['dropout'],
        loss_type=config['model'].get('loss_type', 'mse'),
        use_gradient_checkpointing=config['training'].get('use_gradient_checkpointing', False),
        predictor_config=config.get('v5_optimization', {}).get('advanced_prediction'),
        use_dynamic_graph=config['model'].get('use_dynamic_graph', False),
        k_dynamic_neighbors=config['model'].get('k_dynamic_neighbors', 10),
        num_subjects=num_subjects,
    )

    if num_subjects > 0:
        logger.info(
            f"è¢«è¯•ç‰¹å¼‚æ€§åµŒå…¥å·²å¯ç”¨: {num_subjects} ä¸ªè¢«è¯• Ã— "
            f"{config['model']['hidden_channels']} ç»´ (AGENTS.md Â§ä¹ Gap 2)"
        )
    logger.info(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    return model


def log_training_summary(
    config: dict,
    graphs: list,
    model,
    logger: logging.Logger,
) -> None:
    """å¯åŠ¨è®­ç»ƒå‰æ‰“å°ä¸€æ¬¡äººç±»å¯è¯»çš„é…ç½®æ ¸å¯¹è¡¨ã€‚

    ç›®çš„ï¼šè®©ä»»ä½•äººï¼ˆä¸éœ€è¦äº†è§£ä»£ç ç»†èŠ‚ï¼‰åœ¨çœ‹åˆ°æ—¥å¿—çš„ç¬¬ä¸€çœ¼å°±èƒ½
    ç¡®è®¤æ•°æ®å¤„ç†æ–¹å¼æ˜¯å¦ç¬¦åˆé¢„æœŸï¼Œé¿å…"é»˜é»˜åœ°ç”¨äº†é”™è¯¯å‚æ•°è®­ç»ƒå®Œ
    æ‰å‘ç°"çš„æƒ…å†µã€‚

    ä¿¡æ¯æ¥æºï¼š
    - ä¼˜å…ˆè¯»å– graphs[0] ä¸­çš„å®é™…è¿è¡Œæ—¶å€¼ï¼ˆèŠ‚ç‚¹æ•°ã€åºåˆ—é•¿åº¦ç­‰ï¼‰ï¼Œ
      è€Œä¸æ˜¯ config ä¸­çš„æœŸæœ›å€¼â€”â€”ä¸¤è€…å¯èƒ½å› æ•°æ®è´¨é‡é—®é¢˜è€Œä¸åŒã€‚
    - è¯»å– config è·å–æ¨¡å‹ç»“æ„å’Œè®­ç»ƒè¶…å‚æ•°ã€‚
    """
    sep = "=" * 60

    logger.info(sep)
    logger.info("ğŸ“‹ è®­ç»ƒé…ç½®æ ¸å¯¹è¡¨ (Training Configuration Summary)")
    logger.info(sep)

    # â”€â”€ ä»ç¬¬ä¸€ä¸ªå›¾æå–è¿è¡Œæ—¶å®é™…å€¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    g = graphs[0] if graphs else None
    modalities = config['data'].get('modalities', [])

    logger.info("ã€æ•°æ®ã€‘")

    if g is not None:
        # EEG
        if 'eeg' in getattr(g, 'node_types', []):
            eeg_x = g['eeg'].x  # [N, T, C]
            N_eeg, T_eeg = eeg_x.shape[0], eeg_x.shape[1]
            has_eeg_pos = (
                hasattr(g['eeg'], 'pos')
                and g['eeg'].pos is not None
                and g['eeg'].pos.shape[0] > 0
            )
            # sampling_rate is always written by map_eeg_to_graph; the fallback
            # here matches that function's default (250 Hz) for robustness.
            eeg_sr = getattr(g['eeg'], 'sampling_rate', 250.0)
            logger.info(
                f"  EEG  : {N_eeg} ä¸ªç”µæé€šé“ | "
                f"é‡‡æ ·ç‡: {eeg_sr:.1f} Hz | "
                f"åºåˆ—é•¿åº¦: {T_eeg} ä¸ªæ—¶é—´ç‚¹"
            )
            pos_note = (
                "å·²æ‰¾åˆ° (æ¥è‡ª MNE standard_1020 montageï¼Œå•ä½ mm)"
                if has_eeg_pos
                else "æœªæ‰¾åˆ° â†’ å°†ä½¿ç”¨éšæœºè·¨æ¨¡æ€è¿æ¥ï¼ˆéè·ç¦»åŠ æƒï¼‰"
            )
            logger.info(f"         ç”µæåæ ‡: {pos_note}")

        # fMRI
        if 'fmri' in getattr(g, 'node_types', []):
            fmri_x = g['fmri'].x  # [N, T, C]
            N_fmri, T_fmri = fmri_x.shape[0], fmri_x.shape[1]
            # sampling_rate is always written by map_fmri_to_graph; the fallback
            # here matches that function's default (0.5 Hz = TR 2 s) for robustness.
            fmri_sr = getattr(g['fmri'], 'sampling_rate', 0.5)
            tr_sec = 1.0 / fmri_sr if fmri_sr > 0 else float('nan')
            atlas_used = N_fmri > 1
            atlas_note = (
                f"å·²å¯ç”¨ ({config['data']['atlas']['name']}, {N_fmri} ä¸ª ROI èŠ‚ç‚¹)"
                if atlas_used
                else f"æœªå¯ç”¨ â†’ å•èŠ‚ç‚¹å›é€€ (N_fmri={N_fmri}ï¼Œç©ºé—´ä¿¡æ¯å·²ä¸¢å¤±)"
            )
            logger.info(
                f"  fMRI : {N_fmri} ä¸ª ROI èŠ‚ç‚¹ | "
                f"é‡‡æ ·ç‡: {fmri_sr:.3g} Hz (TRâ‰ˆ{tr_sec:.1f}s) | "
                f"åºåˆ—é•¿åº¦: {T_fmri} ä¸ªæ—¶é—´ç‚¹"
            )
            logger.info(f"         å›¾è°±åˆ†åŒº: {atlas_note}")
            if not atlas_used:
                logger.info(
                    f"  âš ï¸   fMRI åªæœ‰ {N_fmri} ä¸ªèŠ‚ç‚¹ï¼å›¾å·ç§¯æ— æ³•æå–ç©ºé—´ä¿¡æ¯ã€‚"
                    f" è¯·æ£€æŸ¥ atlas æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ã€nilearn æ˜¯å¦å·²å®‰è£…ã€‚"
                )

        # è·¨æ¨¡æ€è¾¹
        if (
            'eeg' in getattr(g, 'node_types', [])
            and 'fmri' in getattr(g, 'node_types', [])
        ):
            cross_edge_type = ('eeg', 'projects_to', 'fmri')
            if cross_edge_type in getattr(g, 'edge_types', []):
                n_cross = g[cross_edge_type].edge_index.shape[1]
                logger.info(
                    f"  è·¨æ¨¡æ€è¾¹ (EEGâ†’fMRI): {n_cross} æ¡"
                    f" | æ–¹å‘æ­£ç¡® (N_eeg={N_eeg} < N_fmri={N_fmri})"
                    if N_eeg < N_fmri
                    else
                    f"  è·¨æ¨¡æ€è¾¹ (EEGâ†’fMRI): {n_cross} æ¡"
                    f"  âš ï¸  N_eeg({N_eeg}) â‰¥ N_fmri({N_fmri}), è¯·æ£€æŸ¥å›¾è°±åŠ è½½"
                )
            else:
                logger.info("  è·¨æ¨¡æ€è¾¹ (EEGâ†’fMRI): æœªå»ºç«‹")
    else:
        logger.info("  (æ— å›¾æ•°æ®å¯ä¾›åˆ†æ)")

    max_seq = config['training'].get('max_seq_len')
    if max_seq:
        logger.info(f"  åºåˆ—æˆªæ–­ max_seq_len: {max_seq} (é˜²æ­¢ CUDA OOM)")
    else:
        logger.info("  åºåˆ—æˆªæ–­: æœªå¯ç”¨ (è‹¥åºåˆ—è¿‡é•¿å¯èƒ½ OOMï¼Œå»ºè®®è®¾ç½® max_seq_len)")

    # â”€â”€ è®­ç»ƒæ•°æ®ç»„æˆæ‘˜è¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # æ˜¾ç¤ºæ¯ä¸ªè¢«è¯•ã€æ¯ä¸ªä»»åŠ¡çš„æ ·æœ¬æ•°ï¼Œè®©ç”¨æˆ·ä¸€çœ¼çœ‹å‡ºæ•°æ®æ˜¯å¦å‡è¡¡ã€
    # æ˜¯å¦æœ‰è¢«è¯•/ä»»åŠ¡ç¼ºå¤±ï¼Œä»¥åŠè·¨ä»»åŠ¡/è¢«è¯•è®­ç»ƒçš„å®é™…è§„æ¨¡ã€‚
    logger.info("ã€è®­ç»ƒæ•°æ®ç»„æˆã€‘")
    total_samples = len(graphs)
    logger.info(f"  æ€»æ ·æœ¬æ•°: {total_samples}")

    # æŒ‰ä»»åŠ¡ç»Ÿè®¡
    # æ£€æŸ¥ graphs[0] å³å¯ï¼šbuild_graphs() ä¸ºæ‰€æœ‰å›¾ç»Ÿä¸€å†™å…¥ task_idï¼Œ
    # extract_windowed_samples() å°†å…¶ä¼ æ’­åˆ°æ‰€æœ‰çª—å£æ ·æœ¬ï¼Œæ‰€ä»¥è¦ä¹ˆæ‰€æœ‰å›¾éƒ½æœ‰ï¼Œè¦ä¹ˆéƒ½æ²¡æœ‰ã€‚
    if graphs and hasattr(graphs[0], 'task_id'):
        task_counts = Counter(getattr(g, 'task_id', '') for g in graphs)
        logger.info(f"  æŒ‰ä»»åŠ¡åˆ†å¸ƒ ({len(task_counts)} ä¸ªä»»åŠ¡):")
        for task_name, cnt in sorted(task_counts.items()):
            pct = cnt / total_samples * 100
            logger.info(f"    {task_name or '(æ— ä»»åŠ¡å)'}: {cnt} ä¸ªæ ·æœ¬ ({pct:.1f}%)")
    else:
        logger.info("  ä»»åŠ¡åˆ†å¸ƒ: ä¸å¯ç”¨ï¼ˆtask_id æœªå­˜å‚¨ï¼Œè¯·æ¸…é™¤ç¼“å­˜é‡å»ºå›¾ï¼‰")

    # æŒ‰è¢«è¯•ç»Ÿè®¡ï¼ˆåŒä¸Šï¼Œæ£€æŸ¥ graphs[0] å³å¯ï¼‰
    if graphs and hasattr(graphs[0], 'subject_id_str'):
        subj_counts = Counter(getattr(g, 'subject_id_str', '') for g in graphs)
        logger.info(f"  æŒ‰è¢«è¯•åˆ†å¸ƒ ({len(subj_counts)} ä¸ªè¢«è¯•):")
        for subj_id, cnt in sorted(subj_counts.items()):
            pct = cnt / total_samples * 100
            logger.info(f"    {subj_id}: {cnt} ä¸ªæ ·æœ¬ ({pct:.1f}%)")
    else:
        logger.info("  è¢«è¯•åˆ†å¸ƒ: ä¸å¯ç”¨ï¼ˆsubject_id_str æœªå­˜å‚¨ï¼Œè¯·æ¸…é™¤ç¼“å­˜é‡å»ºå›¾ï¼‰")

    # æ•°æ®ç‹¬ç«‹æ€§è¯´æ˜ï¼ˆå…³é”®ï¼šå¸®åŠ©ç”¨æˆ·ç†è§£"å¤šè¢«è¯•å¤šä»»åŠ¡åˆæˆåˆ—è¡¨"çš„è®­ç»ƒè¯­ä¹‰ï¼‰
    logger.info(
        "  âœ… æ•°æ®ç‹¬ç«‹æ€§ç¡®è®¤: æ¯ä¸ªæ ·æœ¬å‡æ¥è‡ªç‹¬ç«‹çš„ (è¢«è¯•, ä»»åŠ¡) ç»„åˆï¼Œ"
        "ä¸åŒè¢«è¯•/ä»»åŠ¡çš„å›¾åœ¨å†…å­˜ä¸­ç›¸äº’ç‹¬ç«‹ï¼Œæ¢¯åº¦æ›´æ–°ä»¥å•æ ·æœ¬ï¼ˆbatch_size=1ï¼‰è¿›è¡Œï¼Œ"
        "ä¸å­˜åœ¨è·¨è¢«è¯•æˆ–è·¨ä»»åŠ¡çš„èŠ‚ç‚¹/ç‰¹å¾æ··åˆã€‚"
    )
    logger.info(
        "  âœ… é€ Epoch æ‰“ä¹±: train_epoch æ¯è½®ä»¥ epoch ç¼–å·ä¸ºç§å­æ‰“ä¹±æ ·æœ¬é¡ºåºï¼Œ"
        "ç¡®ä¿ SGD ä¸ä¼šç³»ç»Ÿæ€§åœ°åå‘åˆ—è¡¨æœ«å°¾çš„è¢«è¯•/ä»»åŠ¡ã€‚"
    )

    # â”€â”€ æ¨¡å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("ã€æ¨¡å‹ã€‘")
    logger.info(
        f"  éšå±‚ç»´åº¦: {config['model']['hidden_channels']} | "
        f"ç¼–ç å±‚æ•°: {config['model']['num_encoder_layers']} | "
        f"è§£ç å±‚æ•°: {config['model']['num_decoder_layers']}"
    )
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"  æ€»å‚æ•°é‡: {total_params:,}")
    logger.info(f"  æŸå¤±å‡½æ•°: {config['model'].get('loss_type', 'mse')}")

    # è¢«è¯•ç‰¹å¼‚æ€§åµŒå…¥ï¼ˆAGENTS.md Â§ä¹ Gap 2 ä¸ªæ€§åŒ–æ•°å­—å­ªç”Ÿï¼‰
    # ç›´æ¥ä»æ¨¡å‹å±æ€§è¯»å–è¿è¡Œæ—¶å€¼ï¼Œè€Œé configï¼Œä»¥è¦†ç›– num_subjects=0 ç­‰é»˜è®¤æƒ…å½¢ã€‚
    num_subjects_rt = getattr(model, 'num_subjects', 0)
    if num_subjects_rt > 0:
        H = config['model']['hidden_channels']
        embed_params = num_subjects_rt * H
        logger.info(
            f"  è¢«è¯•ç‰¹å¼‚æ€§åµŒå…¥: âœ… å·²å¯ç”¨ | "
            f"{num_subjects_rt} ä¸ªè¢«è¯• Ã— {H} ç»´ = {embed_params:,} ä¸ªä¸ªæ€§åŒ–å‚æ•°"
        )
        logger.info(
            f"  ä¸ªæ€§åŒ–åŸç†: æ¯ä¸ªè¢«è¯•å­¦ä¹ ä¸€ä¸ª [H={H}] æ½œç©ºé—´åç§»ï¼Œ"
            f"æ–½åŠ äºæ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾ï¼ˆç¼–ç å™¨è¾“å…¥æŠ•å½±ä¹‹åï¼‰"
        )
        # Verify that graphs carry subject_idx so embedding is actually activated
        has_sidx = graphs and hasattr(graphs[0], 'subject_idx')
        if not has_sidx:
            logger.warning(
                "  âš ï¸  graphs[0] ç¼ºå°‘ subject_idx å±æ€§ï¼"
                " è¢«è¯•åµŒå…¥åœ¨ forward() ä¸­å°†è¢«è·³è¿‡ã€‚"
                " è¿™é€šå¸¸æ„å‘³ç€å›¾ç¼“å­˜æ¥è‡ª V5.18 æˆ–æ›´æ—©ç‰ˆæœ¬ï¼Œ"
                " è¯·æ¸…é™¤ç¼“å­˜ç›®å½•åé‡æ–°è¿è¡Œä»¥é‡å»ºå« subject_idx çš„å›¾ã€‚"
            )
    else:
        logger.info("  è¢«è¯•ç‰¹å¼‚æ€§åµŒå…¥: âŒ æœªå¯ç”¨ (num_subjects=0)")

    # â”€â”€ è®­ç»ƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info("ã€è®­ç»ƒã€‘")
    device = config['device']['type']
    use_amp = config['device'].get('use_amp', True)
    use_gc = config['training'].get('use_gradient_checkpointing', False)
    use_al = config['training'].get('use_adaptive_loss', True)
    lr = config['training']['learning_rate']
    logger.info(
        f"  è®¾å¤‡: {device} | "
        f"æ··åˆç²¾åº¦(AMP): {'æ˜¯' if use_amp else 'å¦'} | "
        f"æ¢¯åº¦æ£€æŸ¥ç‚¹: {'æ˜¯' if use_gc else 'å¦'}"
    )
    logger.info(
        f"  å­¦ä¹ ç‡: {lr} | "
        f"è‡ªé€‚åº”æŸå¤±æƒé‡: {'æ˜¯' if use_al else 'å¦'}"
    )

    logger.info(sep)
    logger.info("âš ï¸  è¯·æ ¸å¯¹ä»¥ä¸Šå‚æ•°æ˜¯å¦ä¸æ‚¨çš„å®éªŒé¢„æœŸä¸€è‡´ï¼Œå†ç»§ç»­è®­ç»ƒã€‚")
    logger.info(sep)


def train_model(model, graphs, config: dict, logger: logging.Logger):
    """è®­ç»ƒæ¨¡å‹"""
    logger.info("=" * 60)
    logger.info("æ­¥éª¤ 4/4: è®­ç»ƒæ¨¡å‹")
    logger.info("=" * 60)
    
    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    if len(graphs) < 2:
        logger.error(f"âŒ æ•°æ®ä¸è¶³: éœ€è¦è‡³å°‘2ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œä½†åªæœ‰ {len(graphs)} ä¸ªæ ·æœ¬")
        logger.error("æç¤º: è¯·å¢åŠ æ•°æ®é‡æˆ–è°ƒæ•´ max_subjects é…ç½®")
        raise ValueError(f"éœ€è¦è‡³å°‘2ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ,ä½†åªæœ‰ {len(graphs)} ä¸ªã€‚è¯·æ£€æŸ¥æ•°æ®é…ç½®ã€‚")
    
    rng = random.Random(42)
    windowed = config.get('windowed_sampling', {}).get('enabled', False)

    if windowed and graphs and hasattr(graphs[0], 'run_idx'):
        # â”€â”€ Run-level åˆ’åˆ†ï¼ˆçª—å£æ¨¡å¼ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # çª—å£æ¨¡å¼ä¸‹ï¼ŒåŒä¸€ run äº§ç”Ÿå¤šä¸ª 50% é‡å çª—å£ã€‚
        # è‹¥ä»¥çª—å£ä¸ºå•ä½éšæœºåˆ’åˆ†ï¼Œæ¥è‡ªåŒä¸€ run çš„é‡å çª—å£ä¼šåŒæ—¶å‡ºç°åœ¨è®­ç»ƒé›†å’Œ
        # éªŒè¯é›†ä¸­ï¼ˆæ•°æ®æ³„æ¼ï¼‰ï¼Œå¯¼è‡´éªŒè¯æŸå¤±è™šä½ã€æ— æ³•åæ˜ çœŸå®æ³›åŒ–æ€§èƒ½ã€‚
        # ä»¥ run ä¸ºå•ä½åˆ’åˆ†ç¡®ä¿ï¼šæŸä¸ª run çš„æ‰€æœ‰çª—å£åªè¿›å…¥è®­ç»ƒé›†æˆ–åªè¿›å…¥éªŒè¯é›†ã€‚
        run_groups: dict = defaultdict(list)
        for g in graphs:
            run_groups[g.run_idx.item()].append(g)
        
        run_keys = sorted(run_groups.keys())
        rng.shuffle(run_keys)
        
        min_val_runs = max(1, len(run_keys) // 10)
        n_train_runs = len(run_keys) - min_val_runs
        if n_train_runs < 1:
            n_train_runs = 1
            min_val_runs = len(run_keys) - 1
        
        train_run_keys = run_keys[:n_train_runs]
        val_run_keys = run_keys[n_train_runs:]
        train_graphs = [g for k in train_run_keys for g in run_groups[k]]
        val_graphs = [g for k in val_run_keys for g in run_groups[k]]
        logger.info(
            f"è®­ç»ƒé›†: {len(train_graphs)} ä¸ªçª—å£ (æ¥è‡ª {len(train_run_keys)} ä¸ª run) | "
            f"éªŒè¯é›†: {len(val_graphs)} ä¸ªçª—å£ (æ¥è‡ª {len(val_run_keys)} ä¸ª run) "
            f"[run-level åˆ’åˆ†ï¼Œé˜²æ­¢é‡å çª—å£æ•°æ®æ³„æ¼ï¼Œseed=42]"
        )
    else:
        # â”€â”€ æ ·æœ¬çº§åˆ’åˆ†ï¼ˆå•æ ·æœ¬æ¨¡å¼æˆ–æ—  run_idxï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # æ‰“ä¹±åå†åˆ’åˆ†ï¼Œé¿å…ä»¥ä¸‹åå·®ï¼š
        # 1. è¢«è¯•æŒ‰å­—æ¯é¡ºåºæ’åˆ—æ—¶æœ€åå‡ ä¸ªè¢«è¯•å…¨éƒ¨åªå‡ºç°åœ¨éªŒè¯é›†
        # 2. å¤šä»»åŠ¡åœºæ™¯ä¸‹æŸç±»ä»»åŠ¡å…¨é›†ä¸­åœ¨ä¸€ç«¯
        shuffled = graphs.copy()
        rng.shuffle(shuffled)
        
        min_val_samples = max(1, len(shuffled) // 10)
        n_train = len(shuffled) - min_val_samples
        if n_train < 1:
            n_train = 1
        train_graphs = shuffled[:n_train]
        val_graphs = shuffled[n_train:]
        logger.info(
            f"è®­ç»ƒé›†: {len(train_graphs)} ä¸ªæ ·æœ¬ | éªŒè¯é›†: {len(val_graphs)} ä¸ªæ ·æœ¬ "
            f"(seed=42 éšæœºæ‰“ä¹±, ç»“æœå¯å¤ç°)"
        )
    
    if len(train_graphs) < 5:
        logger.warning("âš ï¸ è®­ç»ƒæ ·æœ¬è¾ƒå°‘ï¼Œæ¨¡å‹å¯èƒ½è¿‡æ‹Ÿåˆã€‚å»ºè®®ä½¿ç”¨æ›´å¤šæ•°æ®ã€‚")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    logger.info("æ­£åœ¨åˆå§‹åŒ–è®­ç»ƒå™¨...")
    if config['device'].get('use_torch_compile', True):
        logger.info("âš™ï¸ torch.compile() å·²å¯ç”¨ï¼Œé¦–æ¬¡è®­ç»ƒå¯èƒ½éœ€è¦é¢å¤–æ—¶é—´è¿›è¡Œæ¨¡å‹ç¼–è¯‘...")
    
    trainer = GraphNativeTrainer(
        model=model,
        node_types=config['data']['modalities'],
        learning_rate=config['training']['learning_rate'],
        weight_decay=config['training']['weight_decay'],
        use_adaptive_loss=config['training']['use_adaptive_loss'],
        use_eeg_enhancement=config['training']['use_eeg_enhancement'],
        use_amp=config['device'].get('use_amp', True),
        use_gradient_checkpointing=config['training'].get('use_gradient_checkpointing', False),
        use_scheduler=config['training'].get('use_scheduler', True),
        scheduler_type=config['training'].get('scheduler_type', 'cosine'),
        use_torch_compile=config['device'].get('use_torch_compile', True),
        compile_mode=config['device'].get('compile_mode', 'reduce-overhead'),
        device=config['device']['type'],
        optimization_config=config.get('v5_optimization'),
        max_grad_norm=config['training'].get('max_grad_norm', 1.0),
        gradient_accumulation_steps=config['training'].get('gradient_accumulation_steps', 1),
    )
    logger.info("âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    logger.info("=" * 60)
    logger.info("å¼€å§‹è®­ç»ƒå¾ªç¯")
    logger.info("=" * 60)
    
    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    best_epoch = 0
    patience_counter = 0
    no_improvement_warning_shown = False
    epoch_times = []
    output_dir = Path(config['output']['output_dir'])
    best_checkpoint_path = output_dir / "best_model.pt"
    
    for epoch in range(1, config['training']['num_epochs'] + 1):
        epoch_start_time = time.time()
        
        # è®­ç»ƒ
        train_loss = trainer.train_epoch(train_graphs, epoch=epoch, total_epochs=config['training']['num_epochs'])
        
        epoch_time = time.time() - epoch_start_time
        epoch_times.append(epoch_time)
        
        # Estimate remaining time (after first few epochs)
        if len(epoch_times) >= 3:
            avg_epoch_time = sum(epoch_times[-5:]) / len(epoch_times[-5:])  # Use last 5 epochs
            remaining_epochs = config['training']['num_epochs'] - epoch
            eta_seconds = avg_epoch_time * remaining_epochs
            eta_minutes = eta_seconds / 60
            if eta_minutes < 60:
                eta_str = f"{eta_minutes:.1f} åˆ†é’Ÿ"
            else:
                eta_str = f"{eta_minutes/60:.1f} å°æ—¶"
        else:
            eta_str = "è®¡ç®—ä¸­..."
        
        # Memory monitoring every 10 epochs
        if epoch % 10 == 0 and torch.cuda.is_available():
            allocated_gb = torch.cuda.memory_allocated() / 1e9
            reserved_gb = torch.cuda.memory_reserved() / 1e9
            logger.info(f"  ğŸ’¾ GPU Memory: allocated={allocated_gb:.2f} GB, reserved={reserved_gb:.2f} GB")
        
        # Check for NaN loss
        if np.isnan(train_loss) or np.isinf(train_loss):
            logger.error(f"âŒ Training loss is NaN/Inf at epoch {epoch}. Stopping training.")
            raise ValueError("Training diverged: loss is NaN or Inf")
        
        # éªŒè¯
        if epoch % config['training']['val_frequency'] == 0:
            val_loss, r2_dict = trainer.validate(val_graphs)
            
            # Step scheduler based on validation loss (for ReduceLROnPlateau)
            trainer.step_scheduler_on_validation(val_loss)
            
            # Check for NaN validation loss
            if np.isnan(val_loss) or np.isinf(val_loss):
                logger.error(f"âŒ Validation loss is NaN/Inf at epoch {epoch}. Stopping training.")
                raise ValueError("Validation diverged: loss is NaN or Inf")
            
            # Format RÂ² values for logging (show all modalities)
            r2_str = "  ".join(f"{k}={v:.3f}" for k, v in sorted(r2_dict.items()))
            logger.info(
                f"âœ“ Epoch {epoch}/{config['training']['num_epochs']}: "
                f"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, "
                f"{r2_str}, "
                f"time={epoch_time:.1f}s, ETA={eta_str}"
            )
            
            # Warn if no improvement after many epochs
            if epoch >= 50 and best_val_loss == float('inf') and not no_improvement_warning_shown:
                logger.warning("âš ï¸ No improvement in validation loss after 50 epochs. Check data quality and hyperparameters.")
                no_improvement_warning_shown = True
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                improvement = (best_val_loss - val_loss) / best_val_loss * 100 if best_val_loss != float('inf') else 100
                best_val_loss = val_loss
                best_epoch = epoch
                patience_counter = 0
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                trainer.save_checkpoint(best_checkpoint_path, epoch)
                if improvement != 100:
                    logger.info(f"  ğŸ¯ ä¿å­˜æœ€ä½³æ¨¡å‹: val_loss={val_loss:.4f} (æå‡ {improvement:.1f}%)")
                else:
                    logger.info(f"  ğŸ¯ ä¿å­˜æœ€ä½³æ¨¡å‹: val_loss={val_loss:.4f}")
            else:
                patience_counter += 1
            
            # æ—©åœ
            if patience_counter >= config['training']['early_stopping_patience']:
                logger.info(f"â¹ï¸ æ—©åœè§¦å‘: {patience_counter} ä¸ªepochæ— æ”¹è¿›")
                break
        else:
            logger.info(
                f"âœ“ Epoch {epoch}/{config['training']['num_epochs']}: "
                f"train_loss={train_loss:.4f}, time={epoch_time:.1f}s, ETA={eta_str}"
            )
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % config['training']['save_frequency'] == 0:
            checkpoint_path = output_dir / f"checkpoint_epoch_{epoch}.pt"
            trainer.save_checkpoint(checkpoint_path, epoch)
    
    logger.info("è®­ç»ƒå®Œæˆ!")
    logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")

    # â”€â”€ æ¢å¤æœ€ä½³æ¨¡å‹ï¼ˆOptimization 4ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # è®­ç»ƒç»“æŸï¼ˆå«æ—©åœï¼‰åï¼Œtrainer.model å¤„äºæœ€åä¸€ä¸ª epoch çš„çŠ¶æ€ã€‚
    # è‡ªåŠ¨åŠ è½½ best_model.ptï¼Œç¡®ä¿åç»­æ¨ç†/è¯„ä¼°ä½¿ç”¨éªŒè¯é›†æœ€ä¼˜æƒé‡ï¼Œ
    # è€Œéè®­ç»ƒè½¨è¿¹æœ«ç«¯å¯èƒ½å·²è¿‡æ‹Ÿåˆçš„æƒé‡ã€‚
    # è¿™æ˜¯æ‰€æœ‰ç°ä»£è®­ç»ƒæ¡†æ¶ï¼ˆKeras ModelCheckpointã€PyTorch-Lightningï¼‰çš„æ ‡å‡†è¡Œä¸ºã€‚
    if best_checkpoint_path.exists() and best_val_loss < float('inf'):
        try:
            trainer.load_checkpoint(best_checkpoint_path)
            logger.info(
                f"âœ… å·²è‡ªåŠ¨æ¢å¤æœ€ä½³æ¨¡å‹ "
                f"(epoch={best_epoch}, val_loss={best_val_loss:.4f})"
            )
        except Exception as _e:
            logger.warning(
                f"âš ï¸ æœ€ä½³æ¨¡å‹è‡ªåŠ¨æ¢å¤å¤±è´¥: {_e}ã€‚"
                f" å½“å‰æ¨¡å‹ä¸ºæœ€åä¸€ä¸ª epoch çš„çŠ¶æ€ã€‚"
                f" å¯æ‰‹åŠ¨è°ƒç”¨ trainer.load_checkpoint('{best_checkpoint_path}')ã€‚"
            )

    # â”€â”€ SWA é˜¶æ®µï¼ˆOptimization 2ï¼Œå¯é€‰ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # åœ¨ä¸»è®­ç»ƒï¼ˆå«æœ€ä½³æ¨¡å‹æ¢å¤ï¼‰ä¹‹åï¼Œä»¥å›ºå®šä½ LR ç»§ç»­è®­ç»ƒè‹¥å¹² epochï¼Œ
    # å¯¹é€”ä¸­æƒé‡å¿«ç…§å–å¹³å‡ï¼Œæ‰¾åˆ°æ¯” SGD ç»ˆç‚¹æ›´å¹³å¦çš„æå°å€¼ã€‚
    # å‚è€ƒï¼šIzmailov et al. (2018) "Averaging Weights Leads to Wider Optima"
    if config['training'].get('use_swa', False):
        try:
            from torch.optim.swa_utils import AveragedModel, SWALR, update_bn
            _swa_available = True
        except ImportError:
            logger.warning(
                "âš ï¸ torch.optim.swa_utils ä¸å¯ç”¨ï¼ˆéœ€è¦ PyTorch >= 1.6ï¼‰ã€‚"
                " è·³è¿‡ SWA é˜¶æ®µã€‚"
            )
            _swa_available = False

        if _swa_available:
            swa_epochs = int(config['training'].get('swa_epochs', 10))
            swa_lr_ratio = float(config['training'].get('swa_lr_ratio', 0.05))
            swa_lr = config['training']['learning_rate'] * swa_lr_ratio

            logger.info("=" * 60)
            logger.info(f"ğŸ“Š å¼€å§‹ SWA é˜¶æ®µ: {swa_epochs} epochs, LR={swa_lr:.2e}")
            logger.info("=" * 60)

            # 1. Build averaged model wrapper (wraps trainer.model in-place for inference)
            _orig_model = trainer.model  # keep reference for attribute access + compute_loss
            swa_model = AveragedModel(_orig_model)

            # 2. Replace scheduler with constant SWALR for SWA phase
            swa_scheduler = SWALR(
                trainer.optimizer,
                swa_lr=swa_lr,
                anneal_epochs=max(1, swa_epochs // 3),  # smooth transition
                anneal_strategy='cos',
            )

            for swa_ep in range(1, swa_epochs + 1):
                swa_train_loss = trainer.train_epoch(
                    train_graphs,
                    epoch=config['training']['num_epochs'] + swa_ep,
                    total_epochs=config['training']['num_epochs'] + swa_epochs,
                )
                swa_model.update_parameters(trainer.model)
                swa_scheduler.step()
                logger.info(
                    f"  SWA Epoch {swa_ep}/{swa_epochs}: "
                    f"train_loss={swa_train_loss:.4f}"
                )

            # 3. Update BatchNorm statistics using a forward pass over training data.
            #    Required because the SWA-averaged weights never saw batch statistics;
            #    without this step, BatchNorm in GraphNativeDecoder uses stale stats.
            logger.info("  æ›´æ–° SWA æ¨¡å‹ BatchNorm ç»Ÿè®¡é‡...")
            try:
                with torch.no_grad():
                    for bn_data in train_graphs:
                        bn_data_dev = bn_data.to(trainer.device)
                        swa_model(bn_data_dev, return_prediction=False)
            except Exception as _bn_err:
                logger.warning(
                    f"  BatchNorm æ›´æ–°é‡åˆ°é—®é¢˜: {_bn_err}ã€‚"
                    f" SWA æ¨¡å‹ BN ç»Ÿè®¡é‡å¯èƒ½ä¸å‡†ç¡®ã€‚"
                )

            # 4. Validate SWA model directly (without swapping trainer.model
            #    to avoid AveragedModel attribute-proxy issues).
            swa_model.eval()
            swa_total_loss = 0.0
            swa_ss_res: Dict[str, float] = {}
            swa_ss_tot: Dict[str, float] = {}
            with torch.no_grad():
                for val_data in val_graphs:
                    val_data = val_data.to(trainer.device)
                    # AveragedModel.forward() proxies to the wrapped module's forward.
                    # Use the original model for attribute checks like use_prediction.
                    if _orig_model.use_prediction:
                        recon_swa, _, enc_swa = swa_model(
                            val_data, return_prediction=False, return_encoded=True
                        )
                    else:
                        recon_swa, _ = swa_model(val_data, return_prediction=False)
                        enc_swa = None
                    # compute_loss is defined on the underlying module
                    swa_losses = _orig_model.compute_loss(val_data, recon_swa, encoded=enc_swa)
                    swa_total_loss += sum(swa_losses.values()).item()
                    # RÂ² per modality
                    for nt in trainer.node_types:
                        if nt in val_data.node_types and nt in recon_swa:
                            tgt = val_data[nt].x
                            rec = recon_swa[nt]
                            T_min = min(tgt.shape[1], rec.shape[1])
                            tgt = tgt[:, :T_min, :]
                            rec = rec[:, :T_min, :]
                            if rec.shape[0] != tgt.shape[0]:
                                continue
                            tgt_mean = tgt.mean()
                            swa_ss_res[nt] = swa_ss_res.get(nt, 0.0) + ((tgt - rec) ** 2).sum().item()
                            swa_ss_tot[nt] = swa_ss_tot.get(nt, 0.0) + ((tgt - tgt_mean) ** 2).sum().item()
            swa_val_loss = swa_total_loss / max(len(val_graphs), 1)
            swa_r2_dict = {
                f'r2_{nt}': (1.0 - swa_ss_res.get(nt, 0.0) / max(swa_ss_tot.get(nt, 1e-12), 1e-12))
                for nt in trainer.node_types
            }

            swa_r2_str = "  ".join(f"{k}={v:.3f}" for k, v in sorted(swa_r2_dict.items()))
            logger.info(
                f"âœ… SWA å®Œæˆ: val_loss={swa_val_loss:.4f} "
                f"(ä¸»è®­ç»ƒæœ€ä½³: {best_val_loss:.4f})  {swa_r2_str}"
            )
            if swa_val_loss < best_val_loss:
                logger.info(
                    f"  ğŸ¯ SWA éªŒè¯æŸå¤±ä¼˜äºä¸»è®­ç»ƒæœ€ä½³ "
                    f"({swa_val_loss:.4f} < {best_val_loss:.4f})ï¼Œ"
                    f" ä¿å­˜ä¸º swa_model.pt å¹¶æ¨èç”¨äºæ¨ç†ã€‚"
                )
            else:
                logger.info(
                    f"  â„¹ï¸  SWA éªŒè¯æŸå¤± ({swa_val_loss:.4f}) æœªä¼˜äºä¸»è®­ç»ƒæœ€ä½³ "
                    f"({best_val_loss:.4f})ã€‚ä¸¤ç§æ¨¡å‹å‡å·²ä¿å­˜ï¼Œå¯æŒ‰éœ€é€‰æ‹©ã€‚"
                )

            # 5. Save SWA model â€” always save regardless of comparison
            #    (SWA's generalization benefit often shows on held-out test sets)
            swa_checkpoint_path = output_dir / "swa_model.pt"
            try:
                torch.save(swa_model.state_dict(), swa_checkpoint_path)
                logger.info(f"  ğŸ’¾ SWA æ¨¡å‹å·²ä¿å­˜: {swa_checkpoint_path}")
            except Exception as _save_err:
                logger.warning(f"  SWA æ¨¡å‹ä¿å­˜å¤±è´¥: {_save_err}")


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‚æ•°
    parser = argparse.ArgumentParser(description='TwinBrain V5 è®­ç»ƒç³»ç»Ÿ')
    parser.add_argument(
        '--config',
        type=str,
        default=None,
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (default: configs/default.yaml)'
    )
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='éšæœºç§å­ (default: 42)'
    )
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = create_output_dir(
        config['output']['output_dir'],
        config['output']['experiment_name']
    )
    config['output']['output_dir'] = str(output_dir)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(
        output_dir / "training.log",
        level=config['output']['log_level']
    )
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # ä¿å­˜é…ç½®
    save_config(config, output_dir / "config.yaml")
    
    # æ‰“å°é…ç½®
    logger.info("=" * 60)
    logger.info("TwinBrain V5 - å›¾åŸç”Ÿæ•°å­—å­ªç”Ÿè„‘è®­ç»ƒç³»ç»Ÿ")
    logger.info("=" * 60)
    logger.info(f"é…ç½®æ–‡ä»¶: {args.config or 'configs/default.yaml'}")
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    logger.info(f"è®¾å¤‡: {config['device']['type']}")
    logger.info(f"éšæœºç§å­: {args.seed}")
    logger.info("=" * 60)
    
    try:
        # æ­¥éª¤1-2: åŠ è½½æ•°æ® & æ„å»ºå›¾ï¼ˆç¼“å­˜æ„ŸçŸ¥ï¼Œå‘½ä¸­æ—¶è·³è¿‡åŸå§‹æ•°æ®åŠ è½½ï¼‰
        # subject_to_idx: {subject_id_str â†’ int_idx}ï¼Œä¼ ç»™ create_model ä»¥åˆ›å»ºæ­£ç¡®å¤§å°çš„ Embedding
        graphs, mapper, subject_to_idx = build_graphs(config, logger)

        # æŒä¹…åŒ– subject_to_idx æ˜ å°„ï¼Œç¡®ä¿æ¨ç†æ—¶èƒ½å°†è¢«è¯• ID è¿˜åŸåˆ° Embedding ç´¢å¼•ã€‚
        # ä¸ä¿å­˜æ­¤æ–‡ä»¶åˆ™æ— æ³•åœ¨è®­ç»ƒåæ¨ç†æ—¶æ¢å¤æ­£ç¡®çš„ subject_idxï¼Œ
        # å¯¼è‡´è¢«è¯•ç‰¹å¼‚æ€§åµŒå…¥æ— æ³•ä½¿ç”¨ï¼ˆè¯¦è§ SPEC.md Â§2.4ï¼‰ã€‚
        if subject_to_idx:
            sidx_path = Path(config['output']['output_dir']) / "subject_to_idx.json"
            try:
                with open(sidx_path, "w", encoding="utf-8") as _f:
                    json.dump(subject_to_idx, _f, ensure_ascii=False, indent=2)
                logger.info(f"è¢«è¯•ç´¢å¼•æ˜ å°„å·²ä¿å­˜: {sidx_path}")
            except OSError as _e:
                logger.warning(f"ä¿å­˜ subject_to_idx å¤±è´¥ ({sidx_path}): {_e}")

        # æ­¥éª¤3: åˆ›å»ºæ¨¡å‹
        model = create_model(config, logger, num_subjects=len(subject_to_idx))
        
        # å¯åŠ¨å‰æ‰“å°ä¸€æ¬¡äººç±»å¯è¯»çš„é…ç½®æ ¸å¯¹è¡¨ï¼Œæ–¹ä¾¿å¿«é€ŸéªŒè¯å‚æ•°
        log_training_summary(config, graphs, model, logger)
        
        # æ­¥éª¤4: è®­ç»ƒ
        train_model(model, graphs, config, logger)
        
        logger.info("=" * 60)
        logger.info("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"âŒ è¿è¡Œå¤±è´¥: {e}", exc_info=True)
        raise


if __name__ == '__main__':
    main()
